{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        name                                           url  \\\n",
      "0  scikit-learn/scikit-learn  https://github.com/scikit-learn/scikit-learn   \n",
      "1          onlook-dev/onlook          https://github.com/onlook-dev/onlook   \n",
      "2       python-poetry/poetry       https://github.com/python-poetry/poetry   \n",
      "3      evidence-dev/evidence      https://github.com/evidence-dev/evidence   \n",
      "4            7PH/powerglitch            https://github.com/7PH/powerglitch   \n",
      "\n",
      "                                         description    language  stars  \\\n",
      "0           scikit-learn: machine learning in Python      Python  62514   \n",
      "1  The Cursor for Designers • An Open-Source Visu...  TypeScript  20044   \n",
      "2  Python packaging and dependency management mad...      Python  33392   \n",
      "3  Business intelligence as code: build fast, int...  JavaScript   5338   \n",
      "4      Tiny JS library to glitch anything on the web  TypeScript   1389   \n",
      "\n",
      "   forks            updated_at  \n",
      "0  26008  2025-07-02T17:30:25Z  \n",
      "1   1296  2025-07-02T20:09:53Z  \n",
      "2   2350  2025-06-30T20:04:16Z  \n",
      "3    268  2025-06-26T14:58:31Z  \n",
      "4     15  2025-01-16T14:31:58Z  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_github_list(list_url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(list_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to load page: {response.status_code}\")\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    repo_blocks = soup.select('div#user-list-repositories > div.border-bottom')\n",
    "\n",
    "    data = []\n",
    "    for block in repo_blocks:\n",
    "        name_tag = block.select_one('h3 a')\n",
    "        full_name = name_tag['href'].strip('/') if name_tag else 'N/A'\n",
    "        repo_url = f\"https://github.com/{full_name}\"\n",
    "\n",
    "        desc_tag = block.select_one('[itemprop=description]')\n",
    "        description = desc_tag.text.strip() if desc_tag else ''\n",
    "\n",
    "        lang_tag = block.select_one('[itemprop=programmingLanguage]')\n",
    "        language = lang_tag.text.strip() if lang_tag else ''\n",
    "\n",
    "        stars_tag = block.select_one('a[href$=\"/stargazers\"]')\n",
    "        stars = stars_tag.text.strip().replace(',', '') if stars_tag else '0'\n",
    "\n",
    "        forks_tag = block.select_one('a[href$=\"/forks\"]')\n",
    "        forks = forks_tag.text.strip().replace(',', '') if forks_tag else '0'\n",
    "\n",
    "        updated_tag = block.select_one('relative-time')\n",
    "        updated = updated_tag['datetime'] if updated_tag else ''\n",
    "\n",
    "        data.append({\n",
    "            'name': full_name,\n",
    "            'url': repo_url,\n",
    "            'description': description,\n",
    "            'language': language,\n",
    "            'stars': int(stars),\n",
    "            'forks': int(forks),\n",
    "            'updated_at': updated\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Example usage:\n",
    "df_list = scrape_github_list(\"https://github.com/stars/Veatec22/lists/stack\")\n",
    "print(df_list.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "      <th>description</th>\n",
       "      <th>language</th>\n",
       "      <th>stars</th>\n",
       "      <th>forks</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scikit-learn/scikit-learn</td>\n",
       "      <td>https://github.com/scikit-learn/scikit-learn</td>\n",
       "      <td>scikit-learn: machine learning in Python</td>\n",
       "      <td>Python</td>\n",
       "      <td>62514</td>\n",
       "      <td>26008</td>\n",
       "      <td>2025-07-02T17:30:25Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>onlook-dev/onlook</td>\n",
       "      <td>https://github.com/onlook-dev/onlook</td>\n",
       "      <td>The Cursor for Designers • An Open-Source Visu...</td>\n",
       "      <td>TypeScript</td>\n",
       "      <td>20044</td>\n",
       "      <td>1296</td>\n",
       "      <td>2025-07-02T20:09:53Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>python-poetry/poetry</td>\n",
       "      <td>https://github.com/python-poetry/poetry</td>\n",
       "      <td>Python packaging and dependency management mad...</td>\n",
       "      <td>Python</td>\n",
       "      <td>33392</td>\n",
       "      <td>2350</td>\n",
       "      <td>2025-06-30T20:04:16Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>evidence-dev/evidence</td>\n",
       "      <td>https://github.com/evidence-dev/evidence</td>\n",
       "      <td>Business intelligence as code: build fast, int...</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td>5338</td>\n",
       "      <td>268</td>\n",
       "      <td>2025-06-26T14:58:31Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7PH/powerglitch</td>\n",
       "      <td>https://github.com/7PH/powerglitch</td>\n",
       "      <td>Tiny JS library to glitch anything on the web</td>\n",
       "      <td>TypeScript</td>\n",
       "      <td>1389</td>\n",
       "      <td>15</td>\n",
       "      <td>2025-01-16T14:31:58Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lucide-icons/lucide</td>\n",
       "      <td>https://github.com/lucide-icons/lucide</td>\n",
       "      <td>Beautiful &amp; consistent icon toolkit made by th...</td>\n",
       "      <td>TypeScript</td>\n",
       "      <td>17952</td>\n",
       "      <td>877</td>\n",
       "      <td>2025-06-30T19:59:20Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>optuna/optuna</td>\n",
       "      <td>https://github.com/optuna/optuna</td>\n",
       "      <td>A hyperparameter optimization framework</td>\n",
       "      <td>Python</td>\n",
       "      <td>12221</td>\n",
       "      <td>1128</td>\n",
       "      <td>2025-07-02T10:22:41Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name                                           url  \\\n",
       "0  scikit-learn/scikit-learn  https://github.com/scikit-learn/scikit-learn   \n",
       "1          onlook-dev/onlook          https://github.com/onlook-dev/onlook   \n",
       "2       python-poetry/poetry       https://github.com/python-poetry/poetry   \n",
       "3      evidence-dev/evidence      https://github.com/evidence-dev/evidence   \n",
       "4            7PH/powerglitch            https://github.com/7PH/powerglitch   \n",
       "5        lucide-icons/lucide        https://github.com/lucide-icons/lucide   \n",
       "6              optuna/optuna              https://github.com/optuna/optuna   \n",
       "\n",
       "                                         description    language  stars  \\\n",
       "0           scikit-learn: machine learning in Python      Python  62514   \n",
       "1  The Cursor for Designers • An Open-Source Visu...  TypeScript  20044   \n",
       "2  Python packaging and dependency management mad...      Python  33392   \n",
       "3  Business intelligence as code: build fast, int...  JavaScript   5338   \n",
       "4      Tiny JS library to glitch anything on the web  TypeScript   1389   \n",
       "5  Beautiful & consistent icon toolkit made by th...  TypeScript  17952   \n",
       "6            A hyperparameter optimization framework      Python  12221   \n",
       "\n",
       "   forks            updated_at  \n",
       "0  26008  2025-07-02T17:30:25Z  \n",
       "1   1296  2025-07-02T20:09:53Z  \n",
       "2   2350  2025-06-30T20:04:16Z  \n",
       "3    268  2025-06-26T14:58:31Z  \n",
       "4     15  2025-01-16T14:31:58Z  \n",
       "5    877  2025-06-30T19:59:20Z  \n",
       "6   1128  2025-07-02T10:22:41Z  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub Lists Configuration\n",
    "# This file contains the dictionary of GitHub starred lists to be used as tags\n",
    "\n",
    "GITHUB_LISTS = {\n",
    "    \"stack\": {\n",
    "        \"url\": \"https://github.com/stars/Veatec22/lists/stack\",\n",
    "        \"description\": \"Core development stack and essential tools\"\n",
    "    },\n",
    "    \"nice-to-have\": {\n",
    "        \"url\": \"https://github.com/stars/Veatec22/lists/nice-to-have\", \n",
    "        \"description\": \"Useful tools and libraries for future consideration\"\n",
    "    },\n",
    "    \"future-ideas\": {\n",
    "        \"url\": \"https://github.com/stars/Veatec22/lists/future-ideas\",\n",
    "        \"description\": \"Innovative projects and experimental technologies\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# List of tag names for easy iteration\n",
    "TAG_NAMES = list(GITHUB_LISTS.keys())\n",
    "\n",
    "# Default sheet tab name for the combined lists data\n",
    "LISTS_SHEET_TAB = \"lists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting GitHub starred lists sync with tags...\n",
      "⏰ Started at: 2025-07-03T18:37:28.370291\n",
      "🚀 Starting to fetch all GitHub lists...\n",
      "📋 Lists to process: stack, nice-to-have, future-ideas\n",
      "🔍 Scraping list 'stack': https://github.com/stars/Veatec22/lists/stack\n",
      "✅ Found 14 repositories in 'stack' list\n",
      "🔍 Scraping list 'nice-to-have': https://github.com/stars/Veatec22/lists/nice-to-have\n",
      "✅ Found 1 repositories in 'nice-to-have' list\n",
      "🔍 Scraping list 'future-ideas': https://github.com/stars/Veatec22/lists/future-ideas\n",
      "✅ Found 17 repositories in 'future-ideas' list\n",
      "📊 Total repositories fetched: 32\n",
      "🔄 Combining repositories and merging tags...\n",
      "✅ Combined 32 unique repositories with tags\n",
      "📤 Uploading to Google Sheet: github_data, tab: lists\n",
      "📝 Created new worksheet: lists\n",
      "✅ Uploaded 32 rows to Google Sheet: github_data/lists\n",
      "🔗 Sheet URL: https://docs.google.com/spreadsheets/d/1sC--EoeGVjOfcKjeI5U9jL55nN0LhZXD4sepUTqCE20\n",
      "\n",
      "📈 Summary:\n",
      "   • Total unique repositories: 32\n",
      "   • Lists processed: stack, nice-to-have, future-ideas\n",
      "   • Tag distribution:\n",
      "     - future-ideas: 17 repos\n",
      "     - nice-to-have: 1 repos\n",
      "     - stack: 14 repos\n",
      "\n",
      "🎉 Successfully synced GitHub lists!\n",
      "📊 Data available at: https://docs.google.com/spreadsheets/d/1sC--EoeGVjOfcKjeI5U9jL55nN0LhZXD4sepUTqCE20\n",
      "✅ Completed at: 2025-07-03T18:37:39.434983\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "GitHub Starred Lists Fetcher with Tags\n",
    "Fetches repositories from multiple GitHub starred lists and combines them with tags\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import our lists configuration\n",
    "# === CONFIGURATION ===\n",
    "load_dotenv()\n",
    "GCP_CREDENTIALS = os.getenv('GCP_CREDENTIALS')\n",
    "GOOGLE_SHEET_NAME = os.getenv('GOOGLE_SHEET_NAME')\n",
    "GOOGLE_SHEET_ID = os.getenv('GOOGLE_SHEET_ID')\n",
    "\n",
    "def scrape_github_list(list_url, tag_name):\n",
    "    \"\"\"Scrape a single GitHub list and return repository data with tag\"\"\"\n",
    "    print(f\"🔍 Scraping list '{tag_name}': {list_url}\")\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(list_url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"❌ Failed to load page for {tag_name}: {response.status_code}\")\n",
    "            return []\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        repo_blocks = soup.select('div#user-list-repositories > div.border-bottom')\n",
    "\n",
    "        data = []\n",
    "        for block in repo_blocks:\n",
    "            name_tag = block.select_one('h3 a')\n",
    "            if not name_tag:\n",
    "                continue\n",
    "                \n",
    "            full_name = name_tag['href'].strip('/')\n",
    "            repo_url = f\"https://github.com/{full_name}\"\n",
    "\n",
    "            desc_tag = block.select_one('[itemprop=description]')\n",
    "            description = desc_tag.text.strip() if desc_tag else ''\n",
    "\n",
    "            lang_tag = block.select_one('[itemprop=programmingLanguage]')\n",
    "            language = lang_tag.text.strip() if lang_tag else ''\n",
    "\n",
    "            stars_tag = block.select_one('a[href$=\"/stargazers\"]')\n",
    "            stars_text = stars_tag.get_text().strip().replace(',', '') if stars_tag else '0'\n",
    "            \n",
    "            forks_tag = block.select_one('a[href$=\"/forks\"]')\n",
    "            forks_text = forks_tag.get_text().strip().replace(',', '') if forks_tag else '0'\n",
    "\n",
    "            updated_tag = block.select_one('relative-time')\n",
    "            updated = updated_tag['datetime'] if updated_tag else ''\n",
    "\n",
    "            # Parse stars and forks (handle 'k' suffix)\n",
    "            stars = parse_number_with_suffix(stars_text)\n",
    "            forks = parse_number_with_suffix(forks_text)\n",
    "\n",
    "            data.append({\n",
    "                'name': full_name,\n",
    "                'url': repo_url,\n",
    "                'description': description,\n",
    "                'language': language,\n",
    "                'stars': stars,\n",
    "                'forks': forks,\n",
    "                'updated_at': updated,\n",
    "                'tag': tag_name\n",
    "            })\n",
    "\n",
    "        print(f\"✅ Found {len(data)} repositories in '{tag_name}' list\")\n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error scraping {tag_name}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def parse_number_with_suffix(text):\n",
    "    \"\"\"Parse numbers that might have 'k' suffix (e.g., '1.2k' -> 1200)\"\"\"\n",
    "    if not text or text == '0':\n",
    "        return 0\n",
    "    \n",
    "    text = text.lower().strip()\n",
    "    if text.endswith('k'):\n",
    "        try:\n",
    "            return int(float(text[:-1]) * 1000)\n",
    "        except ValueError:\n",
    "            return 0\n",
    "    \n",
    "    try:\n",
    "        return int(text)\n",
    "    except ValueError:\n",
    "        return 0\n",
    "\n",
    "def combine_repos_with_tags(all_repos_data):\n",
    "    \"\"\"Combine repositories from multiple lists and concatenate tags\"\"\"\n",
    "    print(\"🔄 Combining repositories and merging tags...\")\n",
    "    \n",
    "    # Group repos by name (full_name)\n",
    "    repo_dict = {}\n",
    "    \n",
    "    # Process all repos and collect tags\n",
    "    for repo_data in all_repos_data:\n",
    "        repo_name = repo_data['name']\n",
    "        tag = repo_data['tag']\n",
    "        \n",
    "        # Initialize repo entry if not exists\n",
    "        if repo_name not in repo_dict:\n",
    "            repo_dict[repo_name] = {\n",
    "                'tags': set(),\n",
    "                'data': None\n",
    "            }\n",
    "        \n",
    "        # Add tag to this repo\n",
    "        repo_dict[repo_name]['tags'].add(tag)\n",
    "        \n",
    "        # Store repo data (use the first occurrence or update with more recent)\n",
    "        if repo_dict[repo_name]['data'] is None:\n",
    "            repo_dict[repo_name]['data'] = repo_data.copy()\n",
    "            del repo_dict[repo_name]['data']['tag']  # Remove individual tag\n",
    "        else:\n",
    "            # Update with more recent data if available\n",
    "            existing = repo_dict[repo_name]['data']\n",
    "            if repo_data.get('updated_at') and existing.get('updated_at'):\n",
    "                if repo_data['updated_at'] > existing['updated_at']:\n",
    "                    repo_dict[repo_name]['data'] = repo_data.copy()\n",
    "                    del repo_dict[repo_name]['data']['tag']\n",
    "    \n",
    "    # Create final combined list\n",
    "    combined_data = []\n",
    "    for repo_name, repo_info in repo_dict.items():\n",
    "        repo_data = repo_info['data']\n",
    "        tags_list = sorted(list(repo_info['tags']))\n",
    "        \n",
    "        repo_data['tags'] = ', '.join(tags_list)\n",
    "        repo_data['tags_count'] = len(tags_list)\n",
    "        repo_data['fetched_at'] = datetime.now().isoformat()\n",
    "        \n",
    "        combined_data.append(repo_data)\n",
    "    \n",
    "    print(f\"✅ Combined {len(combined_data)} unique repositories with tags\")\n",
    "    \n",
    "    # Sort by stars descending\n",
    "    combined_data.sort(key=lambda x: x.get('stars', 0), reverse=True)\n",
    "    \n",
    "    return pd.DataFrame(combined_data)\n",
    "\n",
    "def fetch_all_lists():\n",
    "    \"\"\"Fetch repositories from all configured GitHub lists\"\"\"\n",
    "    print(\"🚀 Starting to fetch all GitHub lists...\")\n",
    "    print(f\"📋 Lists to process: {', '.join(TAG_NAMES)}\")\n",
    "    \n",
    "    all_repos = []\n",
    "    \n",
    "    for tag_name in TAG_NAMES:\n",
    "        list_config = GITHUB_LISTS[tag_name]\n",
    "        list_url = list_config['url']\n",
    "        \n",
    "        repos = scrape_github_list(list_url, tag_name)\n",
    "        all_repos.extend(repos)\n",
    "        \n",
    "        # Be nice to GitHub\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(f\"📊 Total repositories fetched: {len(all_repos)}\")\n",
    "    \n",
    "    # Combine and process\n",
    "    if all_repos:\n",
    "        return combine_repos_with_tags(all_repos)\n",
    "    else:\n",
    "        print(\"⚠️ No repositories found in any list\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def upload_to_google_sheet(df, sheet_name=GOOGLE_SHEET_NAME, tab_name=LISTS_SHEET_TAB):\n",
    "    \"\"\"Upload DataFrame to Google Sheets\"\"\"\n",
    "    print(f\"📤 Uploading to Google Sheet: {sheet_name}, tab: {tab_name}\")\n",
    "    \n",
    "    # Scope for Sheets + Drive\n",
    "    scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "    \n",
    "    try:\n",
    "        # Validate credentials\n",
    "        if not GCP_CREDENTIALS:\n",
    "            raise ValueError(\"GCP_CREDENTIALS environment variable is not set\")\n",
    "        \n",
    "        # Load credentials from environment variable (JSON string)\n",
    "        if GCP_CREDENTIALS.startswith('{'):\n",
    "            # JSON string\n",
    "            creds_dict = json.loads(GCP_CREDENTIALS)\n",
    "        else:\n",
    "            # File path\n",
    "            with open(GCP_CREDENTIALS, 'r') as f:\n",
    "                creds_dict = json.load(f)\n",
    "        \n",
    "        creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)\n",
    "        client = gspread.authorize(creds)\n",
    "        \n",
    "        # Open (or create) spreadsheet\n",
    "        try:\n",
    "            sheet = client.open(sheet_name)\n",
    "        except gspread.SpreadsheetNotFound:\n",
    "            sheet = client.create(sheet_name)\n",
    "            print(f\"📝 Created new spreadsheet: {sheet_name}\")\n",
    "\n",
    "        # Try to get the worksheet, create if it doesn't exist\n",
    "        try:\n",
    "            worksheet = sheet.worksheet(tab_name)\n",
    "        except gspread.WorksheetNotFound:\n",
    "            worksheet = sheet.add_worksheet(title=tab_name, rows=\"1000\", cols=\"20\")\n",
    "            print(f\"📝 Created new worksheet: {tab_name}\")\n",
    "\n",
    "        # Clear the sheet and upload new data\n",
    "        worksheet.clear()\n",
    "        set_with_dataframe(worksheet, df)\n",
    "        \n",
    "        print(f\"✅ Uploaded {len(df)} rows to Google Sheet: {sheet_name}/{tab_name}\")\n",
    "        print(f\"🔗 Sheet URL: https://docs.google.com/spreadsheets/d/{sheet.id}\")\n",
    "        \n",
    "        return sheet.id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error uploading to Google Sheet: {type(e).__name__}: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"🚀 Starting GitHub starred lists sync with tags...\")\n",
    "    print(f\"⏰ Started at: {datetime.now().isoformat()}\")\n",
    "    \n",
    "    try:\n",
    "        # Fetch all lists\n",
    "        df = fetch_all_lists()\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"⚠️ No repositories found in any list\")\n",
    "            return\n",
    "        \n",
    "        # Upload to Google Sheets\n",
    "        sheet_id = upload_to_google_sheet(df)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\n📈 Summary:\")\n",
    "        print(f\"   • Total unique repositories: {len(df)}\")\n",
    "        print(f\"   • Lists processed: {', '.join(TAG_NAMES)}\")\n",
    "        \n",
    "        # Show tag distribution\n",
    "        tag_counts = {}\n",
    "        for _, row in df.iterrows():\n",
    "            for tag in row['tags'].split(', '):\n",
    "                tag_counts[tag] = tag_counts.get(tag, 0) + 1\n",
    "        \n",
    "        print(f\"   • Tag distribution:\")\n",
    "        for tag, count in sorted(tag_counts.items()):\n",
    "            print(f\"     - {tag}: {count} repos\")\n",
    "        \n",
    "        print(f\"\\n🎉 Successfully synced GitHub lists!\")\n",
    "        print(f\"📊 Data available at: https://docs.google.com/spreadsheets/d/{sheet_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(f\"✅ Completed at: {datetime.now().isoformat()}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
