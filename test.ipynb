{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub Lists Configuration\n",
    "# This file contains the dictionary of GitHub starred lists to be used as tags\n",
    "\n",
    "GITHUB_LISTS = {\n",
    "    \"stack\": {\n",
    "        \"url\": \"https://github.com/stars/Veatec22/lists/stack\",\n",
    "        \"description\": \"Core development stack and essential tools\"\n",
    "    },\n",
    "    \"nice-to-have\": {\n",
    "        \"url\": \"https://github.com/stars/Veatec22/lists/nice-to-have\", \n",
    "        \"description\": \"Useful tools and libraries for future consideration\"\n",
    "    },\n",
    "    \"future-ideas\": {\n",
    "        \"url\": \"https://github.com/stars/Veatec22/lists/future-ideas\",\n",
    "        \"description\": \"Innovative projects and experimental technologies\"\n",
    "    },\n",
    "    \"ignore\": {\n",
    "        \"url\": \"https://github.com/stars/Veatec22/lists/ignore\",\n",
    "        \"description\": \"Repositories to ignore\"\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "# List of tag names for easy iteration\n",
    "TAG_NAMES = list(GITHUB_LISTS.keys())\n",
    "\n",
    "# Default sheet tab name for the unified starred data\n",
    "STARRED_SHEET_TAB = \"starred\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing MotherDuck connection...\n",
      "🌐 Using browser authentication for database: github\n",
      "✅ Successfully connected to MotherDuck!\n",
      "✅ Basic query test: (1,)\n",
      "\n",
      "📊 Available databases:\n",
      "  - github\n",
      "  - md_information_schema\n",
      "\n",
      "📋 Tables in github database:\n",
      "  No tables found - run the sync scripts to populate data\n",
      "\n",
      "🎉 MotherDuck connection test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Test script to verify MotherDuck connection\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import duckdb\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "MOTHERDUCK_TOKEN = os.getenv('MOTHERDUCK_TOKEN')\n",
    "MOTHERDUCK_DB = os.getenv('MOTHERDUCK_DB', 'github')\n",
    "\n",
    "def test_motherduck_connection():\n",
    "    \"\"\"Test the MotherDuck connection\"\"\"\n",
    "    print(\"🧪 Testing MotherDuck connection...\")\n",
    "    \n",
    "    try:\n",
    "        # Try to connect to MotherDuck\n",
    "        if MOTHERDUCK_TOKEN:\n",
    "            connection_string = f'md:{MOTHERDUCK_DB}?motherduck_token={MOTHERDUCK_TOKEN}'\n",
    "            print(f\"🔐 Using token authentication for database: {MOTHERDUCK_DB}\")\n",
    "        else:\n",
    "            connection_string = f'md:{MOTHERDUCK_DB}'\n",
    "            print(f\"🌐 Using browser authentication for database: {MOTHERDUCK_DB}\")\n",
    "        \n",
    "        conn = duckdb.connect(connection_string)\n",
    "        print(\"✅ Successfully connected to MotherDuck!\")\n",
    "        \n",
    "        # Test basic functionality\n",
    "        result = conn.execute(\"SELECT 1 as test_value\").fetchone()\n",
    "        print(f\"✅ Basic query test: {result}\")\n",
    "        \n",
    "        # Show databases\n",
    "        print(\"\\n📊 Available databases:\")\n",
    "        databases = conn.execute(\"SHOW DATABASES\").fetchall()\n",
    "        for db in databases:\n",
    "            print(f\"  - {db[0]}\")\n",
    "        \n",
    "        # Check if our tables exist\n",
    "        print(f\"\\n📋 Tables in {MOTHERDUCK_DB} database:\")\n",
    "        try:\n",
    "            tables = conn.execute(\"SHOW TABLES\").fetchall()\n",
    "            if tables:\n",
    "                for table in tables:\n",
    "                    print(f\"  - {table[0]}\")\n",
    "            else:\n",
    "                print(\"  No tables found - run the sync scripts to populate data\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Could not list tables: {e}\")\n",
    "        \n",
    "        conn.close()\n",
    "        print(\"\\n🎉 MotherDuck connection test completed successfully!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error testing MotherDuck connection: {e}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    success = test_motherduck_connection()\n",
    "    if not success:\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting unified starred repositories sync with MotherDuck...\n",
      "⏰ Started at: 2025-07-04T13:49:48.864583\n",
      "🔍 Fetching starred repositories...\n",
      "📦 Fetched page 1 (63 repos)\n",
      "✅ Total starred repositories: 63\n",
      "🏷️ Fetching curated list tags...\n",
      "🔍 Scraping list 'stack': https://github.com/stars/Veatec22/lists/stack\n",
      "✅ Found 22 repositories in 'stack' list\n",
      "🔍 Scraping list 'nice-to-have': https://github.com/stars/Veatec22/lists/nice-to-have\n",
      "✅ Found 4 repositories in 'nice-to-have' list\n",
      "🔍 Scraping list 'future-ideas': https://github.com/stars/Veatec22/lists/future-ideas\n",
      "✅ Found 27 repositories in 'future-ideas' list\n",
      "🔍 Scraping list 'ignore': https://github.com/stars/Veatec22/lists/ignore\n",
      "✅ Found 3 repositories in 'ignore' list\n",
      "🚫 Added 3 repositories to ignore list\n",
      "✅ Collected tags for 53 repositories\n",
      "🚫 Ignoring 3 repositories\n",
      "🔄 Processing repositories and gathering additional data...\n",
      "📊 Processing fastai/fastai (1/63)\n",
      "🚫 Skipping ignored repository: sherlock-project/sherlock\n",
      "🚫 Skipping ignored repository: home-assistant/core\n",
      "📊 Processing scrapy/scrapy (4/63)\n",
      "📊 Processing resemble-ai/chatterbox (5/63)\n",
      "📊 Processing sinaptik-ai/pandas-ai (6/63)\n",
      "📊 Processing aymericdamien/TensorFlow-Examples (7/63)\n",
      "📊 Processing pytorch/pytorch (8/63)\n",
      "📊 Processing AMAI-GmbH/AI-Expert-Roadmap (9/63)\n",
      "📊 Processing ray-project/ray (10/63)\n",
      "📊 Processing Avik-Jain/100-Days-Of-ML-Code (11/63)\n",
      "📊 Processing numpy/numpy (12/63)\n",
      "📊 Processing autogluon/autogluon (13/63)\n",
      "📊 Processing shankarpandala/lazypredict (14/63)\n",
      "📊 Processing psf/black (15/63)\n",
      "📊 Processing pre-commit/pre-commit-hooks (16/63)\n",
      "📊 Processing pylint-dev/pylint (17/63)\n",
      "📊 Processing tadata-org/fastapi_mcp (18/63)\n",
      "📊 Processing GraphiteEditor/Graphite (19/63)\n",
      "🚫 Skipping ignored repository: dataease/dataease\n",
      "📊 Processing getredash/redash (21/63)\n",
      "📊 Processing metabase/metabase (22/63)\n",
      "📊 Processing supabase/supabase (23/63)\n",
      "📊 Processing tensorflow/tensorflow (24/63)\n",
      "📊 Processing ml-tooling/best-of-ml-python (25/63)\n",
      "📊 Processing keras-team/keras (26/63)\n",
      "📊 Processing apache/superset (27/63)\n",
      "📊 Processing ydataai/ydata-profiling (28/63)\n",
      "📊 Processing onlook-dev/onlook (29/63)\n",
      "📊 Processing dbt-labs/dbt-core (30/63)\n",
      "📊 Processing mendableai/firecrawl (31/63)\n",
      "📊 Processing microsoft/ML-For-Beginners (32/63)\n",
      "📊 Processing sdmg15/Best-websites-a-programmer-should-visit (33/63)\n",
      "📊 Processing gitleaks/gitleaks (34/63)\n",
      "📊 Processing eyaltoledano/claude-task-master (35/63)\n",
      "📊 Processing celery/celery (36/63)\n",
      "📊 Processing python-poetry/poetry (37/63)\n",
      "📊 Processing evidence-dev/evidence (38/63)\n",
      "📊 Processing 7PH/powerglitch (39/63)\n",
      "📊 Processing lucide-icons/lucide (40/63)\n",
      "📊 Processing topoteretes/cognee (41/63)\n",
      "📊 Processing mlabonne/llm-course (42/63)\n",
      "📊 Processing optuna/optuna (43/63)\n",
      "📊 Processing coleifer/huey (44/63)\n",
      "📊 Processing unit8co/darts (45/63)\n",
      "📊 Processing modin-project/modin (46/63)\n",
      "📊 Processing dask/dask (47/63)\n",
      "📊 Processing catboost/catboost (48/63)\n",
      "📊 Processing PriorLabs/TabPFN (49/63)\n",
      "📊 Processing stanfordmlgroup/ngboost (50/63)\n",
      "📊 Processing perpetual-ml/perpetual (51/63)\n",
      "📊 Processing duckdb/duckdb (52/63)\n",
      "📊 Processing gradio-app/gradio (53/63)\n",
      "📊 Processing scikit-learn/scikit-learn (54/63)\n",
      "📊 Processing pycaret/pycaret (55/63)\n",
      "📊 Processing reflex-dev/reflex (56/63)\n",
      "📊 Processing Kanaries/pygwalker (57/63)\n",
      "📊 Processing Kanaries/graphic-walker (58/63)\n",
      "📊 Processing Avaiga/taipy (59/63)\n",
      "📊 Processing pola-rs/polars (60/63)\n",
      "📊 Processing pandas-dev/pandas (61/63)\n",
      "📊 Processing evidentlyai/evidently (62/63)\n",
      "📊 Processing streamlit/streamlit (63/63)\n",
      "✅ Processed 60 repositories (filtered out 3 ignored)\n",
      "📤 Uploading to MotherDuck: starred\n",
      "✅ Connected to MotherDuck database: github\n",
      "✅ Uploaded 60 rows to MotherDuck table: starred\n",
      "\n",
      "📈 Portfolio Summary:\n",
      "   • Total starred repositories: 60\n",
      "   • Curated repositories: 53\n",
      "   • Programming languages: 10\n",
      "   • Total stars accumulated: 1,790,670\n",
      "   • Ignored repositories: 3\n",
      "   • Curated tag distribution:\n",
      "     - stack: 22 repos\n",
      "     - nice-to-have: 4 repos\n",
      "     - future-ideas: 27 repos\n",
      "\n",
      "🎉 Successfully synced unified starred repositories to MotherDuck!\n",
      "✅ Completed at: 2025-07-04T13:50:44.828640\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Unified Starred GitHub Repositories Fetcher\n",
    "Fetches starred repositories from GitHub API and merges with curated list tags\n",
    "Creates a comprehensive portfolio view combining detailed repo data with tag organization\n",
    "Now uses MotherDuck (cloud DuckDB) for data storage\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import duckdb\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "load_dotenv()\n",
    "GHUB_TOKEN = os.getenv('GHUB_TOKEN')\n",
    "MOTHERDUCK_TOKEN = os.getenv('MOTHERDUCK_TOKEN')\n",
    "MOTHERDUCK_DB = os.getenv('MOTHERDUCK_DB', 'github')  # Default database name\n",
    "\n",
    "# GitHub API endpoints\n",
    "API_STARRED_URL = 'https://api.github.com/user/starred'\n",
    "API_RELEASES_URL = 'https://api.github.com/repos/{owner}/{repo}/releases/latest'\n",
    "API_TOPICS_URL = 'https://api.github.com/repos/{owner}/{repo}/topics'\n",
    "\n",
    "auth_headers = {\n",
    "    'Authorization': f'token {GHUB_TOKEN}',\n",
    "    'Accept': 'application/vnd.github.v3+json'\n",
    "}\n",
    "\n",
    "topics_headers = {\n",
    "    'Authorization': f'token {GHUB_TOKEN}',\n",
    "    'Accept': 'application/vnd.github.mercy-preview+json'  # Needed to access topics\n",
    "}\n",
    "\n",
    "def get_motherduck_connection():\n",
    "    \"\"\"Get connection to MotherDuck\"\"\"\n",
    "    try:\n",
    "        if MOTHERDUCK_TOKEN:\n",
    "            connection_string = f'md:{MOTHERDUCK_DB}?motherduck_token={MOTHERDUCK_TOKEN}'\n",
    "        else:\n",
    "            # Use browser-based authentication\n",
    "            connection_string = f'md:{MOTHERDUCK_DB}'\n",
    "        \n",
    "        conn = duckdb.connect(connection_string)\n",
    "        print(f\"✅ Connected to MotherDuck database: {MOTHERDUCK_DB}\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error connecting to MotherDuck: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_starred_repos():\n",
    "    \"\"\"Fetch all starred repositories from GitHub API\"\"\"\n",
    "    print(\"🔍 Fetching starred repositories...\")\n",
    "    starred = []\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            API_STARRED_URL, \n",
    "            headers=auth_headers, \n",
    "            params={'per_page': 100, 'page': page}\n",
    "        )\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error fetching starred repos: {response.status_code} - {response.text}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "        starred.extend(data)\n",
    "        page += 1\n",
    "        print(f\"📦 Fetched page {page-1} ({len(data)} repos)\")\n",
    "\n",
    "    print(f\"✅ Total starred repositories: {len(starred)}\")\n",
    "    return starred\n",
    "\n",
    "def get_last_release_date(owner, repo):\n",
    "    \"\"\"Get the last release date for a repository\"\"\"\n",
    "    url = API_RELEASES_URL.format(owner=owner, repo=repo)\n",
    "    response = requests.get(url, headers=auth_headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"published_at\")\n",
    "    elif response.status_code == 404:\n",
    "        return \"No releases\"\n",
    "    else:\n",
    "        return f\"Error: {response.status_code}\"\n",
    "\n",
    "def get_repo_topics(owner, repo):\n",
    "    \"\"\"Get topics for a repository\"\"\"\n",
    "    url = API_TOPICS_URL.format(owner=owner, repo=repo)\n",
    "    response = requests.get(url, headers=topics_headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get('names', [])\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def scrape_github_list(list_url, tag_name):\n",
    "    \"\"\"Scrape a single GitHub list and return repository names with tag\"\"\"\n",
    "    print(f\"🔍 Scraping list '{tag_name}': {list_url}\")\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(list_url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"❌ Failed to load page for {tag_name}: {response.status_code}\")\n",
    "            return []\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        repo_blocks = soup.select('div#user-list-repositories > div.border-bottom')\n",
    "\n",
    "        repo_names = []\n",
    "        for block in repo_blocks:\n",
    "            name_tag = block.select_one('h3 a')\n",
    "            if not name_tag:\n",
    "                continue\n",
    "                \n",
    "            full_name = name_tag['href'].strip('/')\n",
    "            repo_names.append(full_name)\n",
    "\n",
    "        print(f\"✅ Found {len(repo_names)} repositories in '{tag_name}' list\")\n",
    "        return repo_names\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error scraping {tag_name}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def get_curated_tags():\n",
    "    \"\"\"Fetch all curated lists and create a mapping of repo names to tags\"\"\"\n",
    "    print(\"🏷️ Fetching curated list tags...\")\n",
    "    repo_tags = defaultdict(set)\n",
    "    ignore_repos = set()\n",
    "    \n",
    "    for tag_name in TAG_NAMES:\n",
    "        list_config = GITHUB_LISTS[tag_name]\n",
    "        list_url = list_config['url']\n",
    "        \n",
    "        repo_names = scrape_github_list(list_url, tag_name)\n",
    "        \n",
    "        if tag_name == 'ignore':\n",
    "            # Special handling for ignore list\n",
    "            ignore_repos.update(repo_names)\n",
    "            print(f\"🚫 Added {len(repo_names)} repositories to ignore list\")\n",
    "        else:\n",
    "            for repo_name in repo_names:\n",
    "                repo_tags[repo_name].add(tag_name)\n",
    "        \n",
    "        # Be nice to GitHub\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(f\"✅ Collected tags for {len(repo_tags)} repositories\")\n",
    "    print(f\"🚫 Ignoring {len(ignore_repos)} repositories\")\n",
    "    return repo_tags, ignore_repos\n",
    "\n",
    "def process_repositories(repos, repo_tags, ignore_repos):\n",
    "    \"\"\"Process repositories and gather additional data, merging with curated tags\"\"\"\n",
    "    print(\"🔄 Processing repositories and gathering additional data...\")\n",
    "    data = []\n",
    "\n",
    "    for i, repo in enumerate(repos):\n",
    "        full_name = repo['full_name']\n",
    "        \n",
    "        # Skip ignored repositories\n",
    "        if full_name in ignore_repos:\n",
    "            print(f\"🚫 Skipping ignored repository: {full_name}\")\n",
    "            continue\n",
    "            \n",
    "        owner, repo_name = full_name.split('/')\n",
    "        \n",
    "        print(f\"📊 Processing {full_name} ({i+1}/{len(repos)})\")\n",
    "        \n",
    "        # Get additional data\n",
    "        last_release = get_last_release_date(owner, repo_name)\n",
    "        topics = get_repo_topics(owner, repo_name)\n",
    "        \n",
    "        # Get curated tags for this repo\n",
    "        curated_tags = list(repo_tags.get(full_name, set()))\n",
    "        \n",
    "        # Combine GitHub topics and curated tags\n",
    "        all_tags = topics + curated_tags\n",
    "        \n",
    "        data.append({\n",
    "            'name': full_name,\n",
    "            'description': repo.get('description', ''),\n",
    "            'stars': repo['stargazers_count'],\n",
    "            'forks': repo['forks_count'],\n",
    "            'language': repo.get('language', 'Unknown'),\n",
    "            'url': repo['html_url'],\n",
    "            'last_release': last_release,\n",
    "            'topics': \", \".join(topics),\n",
    "            'curated_tags': \", \".join(sorted(curated_tags)),\n",
    "            'all_tags': \", \".join(sorted(all_tags)),\n",
    "            'tags_count': len(all_tags),\n",
    "            'is_curated': len(curated_tags) > 0,\n",
    "            'created_at': repo['created_at'],\n",
    "            'updated_at': repo['updated_at'],\n",
    "            'pushed_at': repo.get('pushed_at', ''),\n",
    "            'open_issues': repo.get('open_issues_count', 0),\n",
    "            'archived': repo.get('archived', False),\n",
    "            'fork': repo.get('fork', False),\n",
    "            'fetched_at': datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    print(f\"✅ Processed {len(data)} repositories (filtered out {len([r for r in repos if r['full_name'] in ignore_repos])} ignored)\")\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def upload_to_motherduck(df, table_name=\"starred\"):\n",
    "    \"\"\"Upload DataFrame to MotherDuck\"\"\"\n",
    "    print(f\"📤 Uploading to MotherDuck: {table_name}\")\n",
    "    \n",
    "    try:\n",
    "        conn = get_motherduck_connection()\n",
    "        \n",
    "        # Create table if it doesn't exist and insert data\n",
    "        conn.execute(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "        conn.execute(f\"CREATE TABLE {table_name} AS SELECT * FROM df\")\n",
    "        \n",
    "        # Verify upload\n",
    "        result = conn.execute(f\"SELECT COUNT(*) FROM {table_name}\").fetchone()\n",
    "        row_count = result[0]\n",
    "        \n",
    "        print(f\"✅ Uploaded {row_count} rows to MotherDuck table: {table_name}\")\n",
    "        \n",
    "        conn.close()\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error uploading to MotherDuck: {type(e).__name__}: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"🚀 Starting unified starred repositories sync with MotherDuck...\")\n",
    "    print(f\"⏰ Started at: {datetime.now().isoformat()}\")\n",
    "    \n",
    "    try:\n",
    "        # Fetch starred repositories\n",
    "        repos = get_starred_repos()\n",
    "        \n",
    "        if not repos:\n",
    "            print(\"⚠️ No starred repositories found\")\n",
    "            return\n",
    "        \n",
    "        # Get curated tags from lists and ignore list\n",
    "        repo_tags, ignore_repos = get_curated_tags()\n",
    "        \n",
    "        # Process repositories with merged data\n",
    "        df = process_repositories(repos, repo_tags, ignore_repos)\n",
    "        \n",
    "        # Upload to MotherDuck\n",
    "        upload_to_motherduck(df)\n",
    "        \n",
    "        # Print summary\n",
    "        curated_count = len(df[df['is_curated'] == True])\n",
    "        languages_count = len(df['language'].unique())\n",
    "        \n",
    "        print(f\"\\n📈 Portfolio Summary:\")\n",
    "        print(f\"   • Total starred repositories: {len(df)}\")\n",
    "        print(f\"   • Curated repositories: {curated_count}\")\n",
    "        print(f\"   • Programming languages: {languages_count}\")\n",
    "        print(f\"   • Total stars accumulated: {df['stars'].sum():,}\")\n",
    "        print(f\"   • Ignored repositories: {len(ignore_repos)}\")\n",
    "        \n",
    "        # Show curated tag distribution\n",
    "        if curated_count > 0:\n",
    "            print(f\"   • Curated tag distribution:\")\n",
    "            for tag in [t for t in TAG_NAMES if t != 'ignore']:\n",
    "                tag_count = len(df[df['curated_tags'].str.contains(tag, na=False)])\n",
    "                if tag_count > 0:\n",
    "                    print(f\"     - {tag}: {tag_count} repos\")\n",
    "        \n",
    "        print(f\"\\n🎉 Successfully synced unified starred repositories to MotherDuck!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(f\"✅ Completed at: {datetime.now().isoformat()}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting optimized repository recommendation sync with MotherDuck...\n",
      "📊 Reading starred repositories from MotherDuck\n",
      "✅ Connected to MotherDuck database: github\n",
      "✅ Read 60 rows from starred table\n",
      "🚫 Getting ignore list...\n",
      "🔍 Scraping list 'ignore': https://github.com/stars/Veatec22/lists/ignore\n",
      "✅ Found 3 repositories in 'ignore' list\n",
      "🚫 Found 3 repositories to ignore\n",
      "🔍 Analyzing topics from starred repositories...\n",
      "📈 Found 391 unique topics from 60 starred repos\n",
      "🔍 Tracking 60 starred repositories for filtering\n",
      "  • python: 35 repos\n",
      "  • machine-learning: 25 repos\n",
      "  • data-science: 20 repos\n",
      "  • deep-learning: 14 repos\n",
      "  • data-analysis: 12 repos\n",
      "  • hacktoberfest: 11 repos\n",
      "  • data-visualization: 9 repos\n",
      "  • llm: 7 repos\n",
      "  • ai: 7 repos\n",
      "  • analytics: 7 repos\n",
      "🎯 Generating recommendations based on topic analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing topics:   0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Searching for topic: 'python' (min 1000 stars)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing topics:   0%|          | 1/391 [00:03<22:49,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📦 Page 1: 100 repos (total: 100)\n",
      "✅ Found 50 repositories for topic 'python'\n",
      "  🔄 Skipping already starred: tensorflow/tensorflow\n",
      "  🔄 Skipping already starred: pytorch/pytorch\n",
      "  🚫 Skipping ignored repository: home-assistant/core\n",
      "  🔄 Skipping already starred: microsoft/ML-For-Beginners\n",
      "  🔄 Skipping already starred: apache/superset\n",
      "  🚫 Skipping ignored repository: sherlock-project/sherlock\n",
      "  🔄 Skipping already starred: keras-team/keras\n",
      "  🔄 Skipping already starred: scikit-learn/scikit-learn\n",
      "  🔄 Skipping already starred: scrapy/scrapy\n",
      "  🔄 Skipping already starred: Avik-Jain/100-Days-Of-ML-Code\n",
      "  🔄 Skipping already starred: pandas-dev/pandas\n",
      "  🔄 Skipping already starred: aymericdamien/TensorFlow-Examples\n",
      "🔎 Searching for topic: 'machine-learning' (min 1000 stars)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing topics:   1%|          | 2/391 [00:06<20:47,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📦 Page 1: 100 repos (total: 100)\n",
      "✅ Found 50 repositories for topic 'machine-learning'\n",
      "  🔄 Skipping already starred: tensorflow/tensorflow\n",
      "  🔄 Skipping already starred: pytorch/pytorch\n",
      "  🔄 Skipping already starred: microsoft/ML-For-Beginners\n",
      "  🔄 Skipping already starred: keras-team/keras\n",
      "  🔄 Skipping already starred: scikit-learn/scikit-learn\n",
      "  🔄 Skipping already starred: mlabonne/llm-course\n",
      "  🔄 Skipping already starred: Avik-Jain/100-Days-Of-ML-Code\n",
      "  🔄 Skipping already starred: aymericdamien/TensorFlow-Examples\n",
      "  🔄 Skipping already starred: streamlit/streamlit\n",
      "  🔄 Skipping already starred: gradio-app/gradio\n",
      "  🔄 Skipping already starred: ray-project/ray\n",
      "  🔄 Skipping already starred: AMAI-GmbH/AI-Expert-Roadmap\n",
      "  🔄 Skipping already starred: fastai/fastai\n",
      "🔎 Searching for topic: 'data-science' (min 1000 stars)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing topics:   1%|          | 3/391 [00:10<22:14,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📦 Page 1: 100 repos (total: 100)\n",
      "✅ Found 50 repositories for topic 'data-science'\n",
      "  🔄 Skipping already starred: microsoft/ML-For-Beginners\n",
      "  🔄 Skipping already starred: apache/superset\n",
      "  🔄 Skipping already starred: keras-team/keras\n",
      "  🔄 Skipping already starred: scikit-learn/scikit-learn\n",
      "  🔄 Skipping already starred: pandas-dev/pandas\n",
      "  🔄 Skipping already starred: streamlit/streamlit\n",
      "  🔄 Skipping already starred: gradio-app/gradio\n",
      "  🔄 Skipping already starred: ray-project/ray\n",
      "  🔄 Skipping already starred: AMAI-GmbH/AI-Expert-Roadmap\n",
      "  🔄 Skipping already starred: ml-tooling/best-of-ml-python\n",
      "  🔄 Skipping already starred: sinaptik-ai/pandas-ai\n",
      "  🔄 Skipping already starred: ydataai/ydata-profiling\n",
      "🔎 Searching for topic: 'deep-learning' (min 1000 stars)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing topics:   1%|          | 4/391 [00:12<19:56,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📦 Page 1: 100 repos (total: 100)\n",
      "✅ Found 50 repositories for topic 'deep-learning'\n",
      "  🔄 Skipping already starred: tensorflow/tensorflow\n",
      "  🔄 Skipping already starred: pytorch/pytorch\n",
      "  🔄 Skipping already starred: keras-team/keras\n",
      "  🔄 Skipping already starred: Avik-Jain/100-Days-Of-ML-Code\n",
      "  🔄 Skipping already starred: aymericdamien/TensorFlow-Examples\n",
      "  🔄 Skipping already starred: streamlit/streamlit\n",
      "  🔄 Skipping already starred: gradio-app/gradio\n",
      "  🔄 Skipping already starred: ray-project/ray\n",
      "  🔄 Skipping already starred: AMAI-GmbH/AI-Expert-Roadmap\n",
      "  🔄 Skipping already starred: fastai/fastai\n",
      "🔎 Searching for topic: 'data-analysis' (min 1000 stars)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing topics:   1%|▏         | 5/391 [00:16<20:47,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📦 Page 1: 100 repos (total: 100)\n",
      "✅ Found 50 repositories for topic 'data-analysis'\n",
      "  🔄 Skipping already starred: apache/superset\n",
      "  🔄 Skipping already starred: scikit-learn/scikit-learn\n",
      "  🔄 Skipping already starred: pandas-dev/pandas\n",
      "  🔄 Skipping already starred: metabase/metabase\n",
      "  🔄 Skipping already starred: streamlit/streamlit\n",
      "  🔄 Skipping already starred: gradio-app/gradio\n",
      "  🔄 Skipping already starred: AMAI-GmbH/AI-Expert-Roadmap\n",
      "  🔄 Skipping already starred: ml-tooling/best-of-ml-python\n",
      "  🔄 Skipping already starred: sinaptik-ai/pandas-ai\n",
      "  🚫 Skipping ignored repository: dataease/dataease\n",
      "  🔄 Skipping already starred: Kanaries/pygwalker\n",
      "  🔄 Skipping already starred: ydataai/ydata-profiling\n",
      "🔎 Searching for topic: 'hacktoberfest' (min 1000 stars)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing topics:   2%|▏         | 6/391 [00:19<20:48,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📦 Page 1: 100 repos (total: 100)\n",
      "✅ Found 50 repositories for topic 'hacktoberfest'\n",
      "  🚫 Skipping ignored repository: home-assistant/core\n",
      "  🔄 Skipping already starred: sdmg15/Best-websites-a-programmer-should-visit\n",
      "  🚫 Skipping ignored repository: sherlock-project/sherlock\n",
      "  🔄 Skipping already starred: scrapy/scrapy\n",
      "🔎 Searching for topic: 'data-visualization' (min 1000 stars)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing topics:   2%|▏         | 7/391 [00:23<21:25,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📦 Page 1: 100 repos (total: 100)\n",
      "✅ Found 50 repositories for topic 'data-visualization'\n",
      "  🔄 Skipping already starred: apache/superset\n",
      "  🔄 Skipping already starred: metabase/metabase\n",
      "  🔄 Skipping already starred: streamlit/streamlit\n",
      "  🔄 Skipping already starred: gradio-app/gradio\n",
      "  🔄 Skipping already starred: ml-tooling/best-of-ml-python\n",
      "  🔄 Skipping already starred: sinaptik-ai/pandas-ai\n",
      "  🚫 Skipping ignored repository: dataease/dataease\n",
      "  🔄 Skipping already starred: Avaiga/taipy\n",
      "🔎 Searching for topic: 'llm' (min 1000 stars)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing topics:   2%|▏         | 8/391 [00:26<20:41,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📦 Page 1: 100 repos (total: 100)\n",
      "✅ Found 50 repositories for topic 'llm'\n",
      "  🔄 Skipping already starred: mlabonne/llm-course\n",
      "  🔄 Skipping already starred: mendableai/firecrawl\n",
      "  🔄 Skipping already starred: ray-project/ray\n",
      "  🔄 Skipping already starred: gitleaks/gitleaks\n",
      "  🔄 Skipping already starred: sinaptik-ai/pandas-ai\n",
      "🔎 Searching for topic: 'ai' (min 1000 stars)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing topics:   2%|▏         | 9/391 [00:29<21:33,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📦 Page 1: 100 repos (total: 100)\n",
      "✅ Found 50 repositories for topic 'ai'\n",
      "  🔄 Skipping already starred: supabase/supabase\n",
      "  🔄 Skipping already starred: mendableai/firecrawl\n",
      "  🔄 Skipping already starred: AMAI-GmbH/AI-Expert-Roadmap\n",
      "🔎 Searching for topic: 'analytics' (min 1000 stars)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing topics:   3%|▎         | 10/391 [00:32<20:59,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📦 Page 1: 100 repos (total: 100)\n",
      "✅ Found 50 repositories for topic 'analytics'\n",
      "  🔄 Skipping already starred: apache/superset\n",
      "  🔄 Skipping already starred: metabase/metabase\n",
      "  🔄 Skipping already starred: duckdb/duckdb\n",
      "  🔄 Skipping already starred: getredash/redash\n",
      "  🔄 Skipping already starred: dbt-labs/dbt-core\n",
      "  🔄 Skipping already starred: modin-project/modin\n",
      "🔎 Searching for topic: 'pandas' (min 1000 stars)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing topics:   3%|▎         | 11/391 [00:35<19:39,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📦 Page 1: 99 repos (total: 99)\n",
      "✅ Found 50 repositories for topic 'pandas'\n",
      "  🔄 Skipping already starred: pandas-dev/pandas\n",
      "  🔄 Skipping already starred: sinaptik-ai/pandas-ai\n",
      "  🔄 Skipping already starred: Kanaries/pygwalker\n",
      "  🔄 Skipping already starred: dask/dask\n",
      "  🔄 Skipping already starred: ydataai/ydata-profiling\n",
      "  🔄 Skipping already starred: modin-project/modin\n",
      "🔎 Searching for topic: 'pytorch' (min 1000 stars)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing topics:   3%|▎         | 12/391 [00:38<18:28,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📦 Page 1: 100 repos (total: 100)\n",
      "✅ Found 50 repositories for topic 'pytorch'\n",
      "  🔄 Skipping already starred: keras-team/keras\n",
      "  🔄 Skipping already starred: ray-project/ray\n",
      "  🔄 Skipping already starred: fastai/fastai\n",
      "  🔄 Skipping already starred: ml-tooling/best-of-ml-python\n",
      "🔎 Searching for topic: 'tensorflow' (min 1000 stars)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing topics:   3%|▎         | 13/391 [00:41<19:00,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📦 Page 1: 100 repos (total: 100)\n",
      "✅ Found 50 repositories for topic 'tensorflow'\n",
      "  🔄 Skipping already starred: tensorflow/tensorflow\n",
      "  🔄 Skipping already starred: keras-team/keras\n",
      "  🔄 Skipping already starred: aymericdamien/TensorFlow-Examples\n",
      "  🔄 Skipping already starred: ray-project/ray\n",
      "  🔄 Skipping already starred: ml-tooling/best-of-ml-python\n",
      "🔎 Searching for topic: 'scikit-learn' (min 1000 stars)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing topics:   4%|▎         | 14/391 [00:44<18:22,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📦 Page 1: 91 repos (total: 91)\n",
      "✅ Found 50 repositories for topic 'scikit-learn'\n",
      "  🔄 Skipping already starred: microsoft/ML-For-Beginners\n",
      "  🔄 Skipping already starred: Avik-Jain/100-Days-Of-ML-Code\n",
      "  🔄 Skipping already starred: ml-tooling/best-of-ml-python\n",
      "  🔄 Skipping already starred: dask/dask\n",
      "  🔄 Skipping already starred: autogluon/autogluon\n",
      "🔎 Searching for topic: 'business-intelligence' (min 1000 stars)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing topics:   4%|▍         | 15/391 [00:45<15:32,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📦 Page 1: 30 repos (total: 30)\n",
      "✅ Found 30 repositories for topic 'business-intelligence'\n",
      "  🔄 Skipping already starred: apache/superset\n",
      "  🔄 Skipping already starred: metabase/metabase\n",
      "  🔄 Skipping already starred: getredash/redash\n",
      "  🚫 Skipping ignored repository: dataease/dataease\n",
      "  🔄 Skipping already starred: dbt-labs/dbt-core\n",
      "  🔄 Skipping already starred: evidence-dev/evidence\n",
      "🔎 Searching for topic: 'gpu' (min 1000 stars)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing topics:   4%|▍         | 15/391 [00:48<20:15,  3.23s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 317\u001b[39m\n\u001b[32m    314\u001b[39m         sys.exit(\u001b[32m1\u001b[39m)\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 296\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    295\u001b[39m \u001b[38;5;66;03m# Get recommendations based on topics\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m recommendations_dict = \u001b[43mget_recommendations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic_counter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarred_repo_full_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_repos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m recommendations_dict:\n\u001b[32m    299\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m⚠️ No recommendations found.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 178\u001b[39m, in \u001b[36mget_recommendations\u001b[39m\u001b[34m(topic_counter, starred_repo_full_names, ignore_repos, min_stars, max_per_topic)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;66;03m# Process topics in order of frequency (most common first)\u001b[39;00m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m topic, frequency \u001b[38;5;129;01min\u001b[39;00m tqdm(topic_counter.most_common(), desc=\u001b[33m\"\u001b[39m\u001b[33mProcessing topics\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    176\u001b[39m     \u001b[38;5;66;03m# print(f\"\\n--- Processing topic: '{topic}' (appears in {frequency} starred repos) ---\")\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     repos = \u001b[43msearch_repositories_by_topic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_stars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_per_topic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m repo \u001b[38;5;129;01min\u001b[39;00m repos:\n\u001b[32m    181\u001b[39m         repo_id = repo[\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 135\u001b[39m, in \u001b[36msearch_repositories_by_topic\u001b[39m\u001b[34m(topic, min_stars, max_results)\u001b[39m\n\u001b[32m    126\u001b[39m params = {\n\u001b[32m    127\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mq\u001b[39m\u001b[33m'\u001b[39m: query,\n\u001b[32m    128\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msort\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mstars\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    131\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mpage\u001b[39m\u001b[33m'\u001b[39m: page\n\u001b[32m    132\u001b[39m }\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttps://api.github.com/search/repositories\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.status_code != \u001b[32m200\u001b[39m:\n\u001b[32m    139\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m⚠️ API error for topic \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Repos\\evidence_test\\.venv\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Repos\\evidence_test\\.venv\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Repos\\evidence_test\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Repos\\evidence_test\\.venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Repos\\evidence_test\\.venv\\Lib\\site-packages\\requests\\adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Repos\\evidence_test\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Repos\\evidence_test\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Repos\\evidence_test\\.venv\\Lib\\site-packages\\urllib3\\connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:1395\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1393\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1394\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1395\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1396\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1397\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:325\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:286\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp.readline(_MAXLINE + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    288\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:706\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    705\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    708\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1314\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1310\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1311\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1312\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1313\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1166\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1164\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1166\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1167\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "GitHub Repository Recommender\n",
    "Recommends GitHub repositories based on user's starred repositories topics.\n",
    "Optimized for speed and relevance.\n",
    "Now uses MotherDuck (cloud DuckDB) for data storage.\n",
    "'''\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import duckdb\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import our lists configuration for ignore list\n",
    "# === CONFIGURATION ===\n",
    "load_dotenv()\n",
    "GHUB_TOKEN = os.getenv('GHUB_TOKEN')\n",
    "MOTHERDUCK_TOKEN = os.getenv('MOTHERDUCK_TOKEN')\n",
    "MOTHERDUCK_DB = os.getenv('MOTHERDUCK_DB', 'github')  # Default database name\n",
    "\n",
    "auth_headers = {\n",
    "    'Authorization': f'token {GHUB_TOKEN}',\n",
    "    'Accept': 'application/vnd.github.v3+json'\n",
    "}\n",
    "\n",
    "def get_motherduck_connection():\n",
    "    \"\"\"Get connection to MotherDuck\"\"\"\n",
    "    try:\n",
    "        if MOTHERDUCK_TOKEN:\n",
    "            connection_string = f'md:{MOTHERDUCK_DB}?motherduck_token={MOTHERDUCK_TOKEN}'\n",
    "        else:\n",
    "            # Use browser-based authentication\n",
    "            connection_string = f'md:{MOTHERDUCK_DB}'\n",
    "        \n",
    "        conn = duckdb.connect(connection_string)\n",
    "        print(f\"✅ Connected to MotherDuck database: {MOTHERDUCK_DB}\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error connecting to MotherDuck: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_starred_repos_from_motherduck():\n",
    "    \"\"\"Read starred repositories from MotherDuck\"\"\"\n",
    "    print(f\"📊 Reading starred repositories from MotherDuck\")\n",
    "    \n",
    "    try:\n",
    "        conn = get_motherduck_connection()\n",
    "        \n",
    "        # Get starred repositories data\n",
    "        query = \"SELECT * FROM starred\"\n",
    "        df = conn.execute(query).df()\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "        print(f\"✅ Read {len(df)} rows from starred table\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading from MotherDuck: {type(e).__name__}: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_ignore_repos():\n",
    "    \"\"\"Get the list of repositories to ignore from the ignore list\"\"\"\n",
    "    print(\"🚫 Getting ignore list...\")\n",
    "    \n",
    "    try:\n",
    "        # Import the scraper function from starred_fetcher\n",
    "        \n",
    "        ignore_config = GITHUB_LISTS.get('ignore')\n",
    "        if not ignore_config:\n",
    "            print(\"⚠️ No ignore list found in configuration\")\n",
    "            return set()\n",
    "        \n",
    "        ignore_repos = scrape_github_list(ignore_config['url'], 'ignore')\n",
    "        ignore_set = set(ignore_repos)\n",
    "        \n",
    "        print(f\"🚫 Found {len(ignore_set)} repositories to ignore\")\n",
    "        return ignore_set\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error getting ignore list: {e}\")\n",
    "        return set()\n",
    "\n",
    "def extract_topic_frequencies(df_starred):\n",
    "    \"\"\"Extract and count topic frequencies from starred repositories\"\"\"\n",
    "    print(\"🔍 Analyzing topics from starred repositories...\")\n",
    "    \n",
    "    all_topics = []\n",
    "    starred_repo_full_names = set()\n",
    "    \n",
    "    for _, row in df_starred.iterrows():\n",
    "        # Collect starred repo full names for filtering (owner/repo format)\n",
    "        if pd.notna(row.get('name')):\n",
    "            starred_repo_full_names.add(row['name'])\n",
    "        \n",
    "        # Extract topics\n",
    "        topics_str = row.get('topics', '')\n",
    "        if pd.notna(topics_str) and topics_str.strip():\n",
    "            topics = [t.strip().lower() for t in topics_str.split(',') if t.strip()]\n",
    "            all_topics.extend(topics)\n",
    "    \n",
    "    # Count topic frequencies\n",
    "    topic_counter = Counter(all_topics)\n",
    "    \n",
    "    print(f\"📈 Found {len(topic_counter)} unique topics from {len(df_starred)} starred repos\")\n",
    "    print(f\"🔍 Tracking {len(starred_repo_full_names)} starred repositories for filtering\")\n",
    "    for topic, count in topic_counter.most_common(10):\n",
    "        print(f\"  • {topic}: {count} repos\")\n",
    "    \n",
    "    return topic_counter, starred_repo_full_names\n",
    "\n",
    "def search_repositories_by_topic(topic, min_stars=1000, max_results=50):\n",
    "    \"\"\"Search GitHub repositories by a specific topic\"\"\"\n",
    "    print(f\"🔎 Searching for topic: '{topic}' (min {min_stars} stars)\")\n",
    "    \n",
    "    repositories = []\n",
    "    per_page = 100\n",
    "    max_pages = (max_results // per_page) + 1\n",
    "    \n",
    "    for page in range(1, max_pages + 1):\n",
    "        query = f\"topic:{topic} stars:>={min_stars}\"\n",
    "        params = {\n",
    "            'q': query,\n",
    "            'sort': 'stars',\n",
    "            'order': 'desc',\n",
    "            'per_page': per_page,\n",
    "            'page': page\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get('https://api.github.com/search/repositories', \n",
    "                                  headers=auth_headers, params=params)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"⚠️ API error for topic '{topic}': {response.status_code}\")\n",
    "                break\n",
    "            \n",
    "            data = response.json()\n",
    "            items = data.get('items', [])\n",
    "            \n",
    "            if not items:\n",
    "                break\n",
    "                \n",
    "            repositories.extend(items)\n",
    "            print(f\"  📦 Page {page}: {len(items)} repos (total: {len(repositories)})\")\n",
    "            \n",
    "            # Stop if we have enough results\n",
    "            if len(repositories) >= max_results:\n",
    "                repositories = repositories[:max_results]\n",
    "                break\n",
    "                \n",
    "            # Rate limiting\n",
    "            time.sleep(0.1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error searching for topic '{topic}': {e}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"✅ Found {len(repositories)} repositories for topic '{topic}'\")\n",
    "    return repositories\n",
    "\n",
    "def get_recommendations(topic_counter, starred_repo_full_names, ignore_repos, min_stars=1000, max_per_topic=50):\n",
    "    \"\"\"Get repository recommendations based on topic frequencies\"\"\"\n",
    "    print(\"🎯 Generating recommendations based on topic analysis...\")\n",
    "    \n",
    "    all_recommendations = {}\n",
    "    filtered_count = 0\n",
    "    ignored_count = 0\n",
    "    \n",
    "    # Process topics in order of frequency (most common first)\n",
    "    for topic, frequency in tqdm(topic_counter.most_common(), desc=\"Processing topics\"):\n",
    "        # print(f\"\\n--- Processing topic: '{topic}' (appears in {frequency} starred repos) ---\")\n",
    "        \n",
    "        repos = search_repositories_by_topic(topic, min_stars, max_per_topic)\n",
    "        \n",
    "        for repo in repos:\n",
    "            repo_id = repo['id']\n",
    "            repo_full_name = repo['full_name']  # This is \"owner/repo\" format\n",
    "            \n",
    "            # Skip if already starred (check against full_name)\n",
    "            if repo_full_name in starred_repo_full_names:\n",
    "                filtered_count += 1\n",
    "                print(f\"  🔄 Skipping already starred: {repo_full_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Skip if in ignore list\n",
    "            if repo_full_name in ignore_repos:\n",
    "                ignored_count += 1\n",
    "                print(f\"  🚫 Skipping ignored repository: {repo_full_name}\")\n",
    "                continue\n",
    "            \n",
    "            # If we haven't seen this repo before, add it\n",
    "            if repo_id not in all_recommendations:\n",
    "                all_recommendations[repo_id] = {\n",
    "                    'repo_data': repo,\n",
    "                    'topic_matches': [],\n",
    "                    'total_frequency': 0\n",
    "                }\n",
    "            \n",
    "            # Add this topic match\n",
    "            all_recommendations[repo_id]['topic_matches'].append(topic)\n",
    "            all_recommendations[repo_id]['total_frequency'] += frequency\n",
    "    \n",
    "    print(f\"\\n✅ Found {len(all_recommendations)} unique recommendations\")\n",
    "    print(f\"🔄 Filtered out {filtered_count} already-starred repositories\")\n",
    "    print(f\"🚫 Filtered out {ignored_count} ignored repositories\")\n",
    "    return all_recommendations\n",
    "\n",
    "def format_recommendations(recommendations_dict):\n",
    "    \"\"\"Format recommendations into a DataFrame sorted by relevance and stars\"\"\"\n",
    "    print(\"📊 Formatting and ranking recommendations...\")\n",
    "    \n",
    "    formatted_recommendations = []\n",
    "    \n",
    "    for repo_id, data in recommendations_dict.items():\n",
    "        repo = data['repo_data']\n",
    "        topic_matches = data['topic_matches']\n",
    "        total_frequency = data['total_frequency']\n",
    "        \n",
    "        formatted_recommendations.append({\n",
    "            'name': repo['full_name'],\n",
    "            'description': repo.get('description', ''),\n",
    "            'stars': repo['stargazers_count'],\n",
    "            'forks': repo['forks_count'],\n",
    "            'language': repo.get('language', 'Unknown'),\n",
    "            'url': repo['html_url'],\n",
    "            'topics': ', '.join(repo.get('topics', [])),\n",
    "            'matched_topics': ', '.join(topic_matches),\n",
    "            'topic_frequency_score': total_frequency,\n",
    "            'num_topic_matches': len(topic_matches)\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(formatted_recommendations)\n",
    "    \n",
    "    # Sort by topic frequency score (descending) and then by stars (descending)\n",
    "    df = df.sort_values(['topic_frequency_score', 'stars'], ascending=[False, False])\n",
    "    \n",
    "    print(f\"🏆 Top 10 recommendations:\")\n",
    "    for i, row in df.head(10).iterrows():\n",
    "        print(f\"  {i+1}. {row['name']} ({row['stars']:,} ⭐) - Score: {row['topic_frequency_score']}\")\n",
    "        print(f\"     Matched topics: {row['matched_topics']}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def upload_recommendations_to_motherduck(df, table_name='recommendations'):\n",
    "    \"\"\"Upload DataFrame to MotherDuck\"\"\"\n",
    "    print(f\"📤 Uploading recommendations to MotherDuck: {table_name}\")\n",
    "    \n",
    "    try:\n",
    "        conn = get_motherduck_connection()\n",
    "        \n",
    "        # Create table if it doesn't exist and insert data\n",
    "        conn.execute(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "        conn.execute(f\"CREATE TABLE {table_name} AS SELECT * FROM df\")\n",
    "        \n",
    "        # Verify upload\n",
    "        result = conn.execute(f\"SELECT COUNT(*) FROM {table_name}\").fetchone()\n",
    "        row_count = result[0]\n",
    "        \n",
    "        print(f\"✅ Uploaded {row_count} rows to MotherDuck table: {table_name}\")\n",
    "        \n",
    "        conn.close()\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error uploading to MotherDuck: {type(e).__name__}: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"🚀 Starting optimized repository recommendation sync with MotherDuck...\")\n",
    "    \n",
    "    try:\n",
    "        # Read starred repositories from MotherDuck\n",
    "        df_starred = get_starred_repos_from_motherduck()\n",
    "        \n",
    "        if df_starred.empty:\n",
    "            print(\"⚠️ No starred repositories found in MotherDuck. Cannot generate recommendations.\")\n",
    "            return\n",
    "        \n",
    "        # Get ignore list\n",
    "        ignore_repos = get_ignore_repos()\n",
    "        \n",
    "        # Extract topic frequencies and starred repo names\n",
    "        topic_counter, starred_repo_full_names = extract_topic_frequencies(df_starred)\n",
    "        \n",
    "        if not topic_counter:\n",
    "            print(\"⚠️ No topics found in starred repositories. Cannot generate recommendations.\")\n",
    "            return\n",
    "        \n",
    "        # Get recommendations based on topics\n",
    "        recommendations_dict = get_recommendations(topic_counter, starred_repo_full_names, ignore_repos)\n",
    "        \n",
    "        if not recommendations_dict:\n",
    "            print(\"⚠️ No recommendations found.\")\n",
    "            return\n",
    "        \n",
    "        # Format and sort recommendations\n",
    "        df_recommendations = format_recommendations(recommendations_dict)\n",
    "        \n",
    "        # Upload recommendations to MotherDuck\n",
    "        upload_recommendations_to_motherduck(df_recommendations)\n",
    "        \n",
    "        print(\"🎉 Successfully generated and uploaded repository recommendations!\")\n",
    "        print(f\"📈 Generated {len(df_recommendations)} recommendations based on {len(topic_counter)} topics\")\n",
    "        print(f\"🚫 Ignored {len(ignore_repos)} repositories from ignore list\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
