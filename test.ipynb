{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting unified starred repositories sync...\n",
      "⏰ Started at: 2025-07-03T21:49:17.027129\n",
      "🔍 Fetching starred repositories...\n",
      "📦 Fetched page 1 (54 repos)\n",
      "✅ Total starred repositories: 54\n",
      "🏷️ Fetching curated list tags...\n",
      "🔍 Scraping list 'stack': https://github.com/stars/Veatec22/lists/stack\n",
      "✅ Found 14 repositories in 'stack' list\n",
      "🔍 Scraping list 'nice-to-have': https://github.com/stars/Veatec22/lists/nice-to-have\n",
      "✅ Found 1 repositories in 'nice-to-have' list\n",
      "🔍 Scraping list 'future-ideas': https://github.com/stars/Veatec22/lists/future-ideas\n",
      "✅ Found 17 repositories in 'future-ideas' list\n",
      "✅ Collected tags for 32 repositories\n",
      "🔄 Processing repositories and gathering additional data...\n",
      "📊 Processing shankarpandala/lazypredict (1/54)\n",
      "📊 Processing psf/black (2/54)\n",
      "📊 Processing pre-commit/pre-commit-hooks (3/54)\n",
      "📊 Processing pylint-dev/pylint (4/54)\n",
      "📊 Processing tadata-org/fastapi_mcp (5/54)\n",
      "📊 Processing GraphiteEditor/Graphite (6/54)\n",
      "📊 Processing dataease/dataease (7/54)\n",
      "📊 Processing getredash/redash (8/54)\n",
      "📊 Processing metabase/metabase (9/54)\n",
      "📊 Processing supabase/supabase (10/54)\n",
      "📊 Processing tensorflow/tensorflow (11/54)\n",
      "📊 Processing ml-tooling/best-of-ml-python (12/54)\n",
      "📊 Processing keras-team/keras (13/54)\n",
      "📊 Processing apache/superset (14/54)\n",
      "📊 Processing ydataai/ydata-profiling (15/54)\n",
      "📊 Processing onlook-dev/onlook (16/54)\n",
      "📊 Processing dbt-labs/dbt-core (17/54)\n",
      "📊 Processing mendableai/firecrawl (18/54)\n",
      "📊 Processing microsoft/ML-For-Beginners (19/54)\n",
      "📊 Processing sdmg15/Best-websites-a-programmer-should-visit (20/54)\n",
      "📊 Processing gitleaks/gitleaks (21/54)\n",
      "📊 Processing eyaltoledano/claude-task-master (22/54)\n",
      "📊 Processing celery/celery (23/54)\n",
      "📊 Processing python-poetry/poetry (24/54)\n",
      "📊 Processing evidence-dev/evidence (25/54)\n",
      "📊 Processing 7PH/powerglitch (26/54)\n",
      "📊 Processing lucide-icons/lucide (27/54)\n",
      "📊 Processing topoteretes/cognee (28/54)\n",
      "📊 Processing mlabonne/llm-course (29/54)\n",
      "📊 Processing optuna/optuna (30/54)\n",
      "📊 Processing coleifer/huey (31/54)\n",
      "📊 Processing unit8co/darts (32/54)\n",
      "📊 Processing modin-project/modin (33/54)\n",
      "📊 Processing dask/dask (34/54)\n",
      "📊 Processing catboost/catboost (35/54)\n",
      "📊 Processing PriorLabs/TabPFN (36/54)\n",
      "📊 Processing elephaint/pgbm (37/54)\n",
      "📊 Processing stanfordmlgroup/ngboost (38/54)\n",
      "📊 Processing LinearBoost/linearboost-classifier (39/54)\n",
      "📊 Processing ThomasMeissnerDS/BlueCast (40/54)\n",
      "📊 Processing perpetual-ml/perpetual (41/54)\n",
      "📊 Processing duckdb/duckdb (42/54)\n",
      "📊 Processing gradio-app/gradio (43/54)\n",
      "📊 Processing scikit-learn/scikit-learn (44/54)\n",
      "📊 Processing pycaret/pycaret (45/54)\n",
      "📊 Processing reflex-dev/reflex (46/54)\n",
      "📊 Processing Kanaries/pygwalker (47/54)\n",
      "📊 Processing Kanaries/graphic-walker (48/54)\n",
      "📊 Processing Avaiga/taipy (49/54)\n",
      "📊 Processing pola-rs/polars (50/54)\n",
      "📊 Processing pandas-dev/pandas (51/54)\n",
      "📊 Processing evidentlyai/evidently (52/54)\n",
      "📊 Processing streamlit/streamlit (53/54)\n",
      "📊 Processing autogluon/autogluon (54/54)\n",
      "✅ Processed 54 repositories\n",
      "📤 Uploading to Google Sheet: github_data, tab: starred\n",
      "✅ Uploaded 54 rows to Google Sheet: github_data/starred\n",
      "🔗 Sheet URL: https://docs.google.com/spreadsheets/d/1sC--EoeGVjOfcKjeI5U9jL55nN0LhZXD4sepUTqCE20\n",
      "\n",
      "📈 Portfolio Summary:\n",
      "   • Total starred repositories: 54\n",
      "   • Curated repositories: 32\n",
      "   • Programming languages: 11\n",
      "   • Total stars accumulated: 1,416,245\n",
      "   • Curated tag distribution:\n",
      "     - stack: 14 repos\n",
      "     - nice-to-have: 1 repos\n",
      "     - future-ideas: 17 repos\n",
      "\n",
      "🎉 Successfully synced unified starred repositories!\n",
      "📊 Portfolio data available at: https://docs.google.com/spreadsheets/d/1sC--EoeGVjOfcKjeI5U9jL55nN0LhZXD4sepUTqCE20\n",
      "✅ Completed at: 2025-07-03T21:50:08.877720\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Unified Starred GitHub Repositories Fetcher\n",
    "Fetches starred repositories from GitHub API and merges with curated list tags\n",
    "Creates a comprehensive portfolio view combining detailed repo data with tag organization\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import our lists configuration  \n",
    "# GitHub Lists Configuration\n",
    "# This file contains the dictionary of GitHub starred lists to be used as tags\n",
    "\n",
    "GITHUB_LISTS = {\n",
    "    \"stack\": {\n",
    "        \"url\": \"https://github.com/stars/Veatec22/lists/stack\",\n",
    "        \"description\": \"Core development stack and essential tools\"\n",
    "    },\n",
    "    \"nice-to-have\": {\n",
    "        \"url\": \"https://github.com/stars/Veatec22/lists/nice-to-have\", \n",
    "        \"description\": \"Useful tools and libraries for future consideration\"\n",
    "    },\n",
    "    \"future-ideas\": {\n",
    "        \"url\": \"https://github.com/stars/Veatec22/lists/future-ideas\",\n",
    "        \"description\": \"Innovative projects and experimental technologies\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# List of tag names for easy iteration\n",
    "TAG_NAMES = list(GITHUB_LISTS.keys())\n",
    "\n",
    "# Default sheet tab name for the unified starred data\n",
    "STARRED_SHEET_TAB = \"starred\"\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "load_dotenv()\n",
    "GHUB_TOKEN = os.getenv('GHUB_TOKEN')\n",
    "GCP_CREDENTIALS = os.getenv('GCP_CREDENTIALS')\n",
    "GOOGLE_SHEET_NAME = os.getenv('GOOGLE_SHEET_NAME')\n",
    "GOOGLE_SHEET_ID = os.getenv('GOOGLE_SHEET_ID')\n",
    "\n",
    "# GitHub API endpoints\n",
    "API_STARRED_URL = 'https://api.github.com/user/starred'\n",
    "API_RELEASES_URL = 'https://api.github.com/repos/{owner}/{repo}/releases/latest'\n",
    "API_TOPICS_URL = 'https://api.github.com/repos/{owner}/{repo}/topics'\n",
    "\n",
    "auth_headers = {\n",
    "    'Authorization': f'token {GHUB_TOKEN}',\n",
    "    'Accept': 'application/vnd.github.v3+json'\n",
    "}\n",
    "\n",
    "topics_headers = {\n",
    "    'Authorization': f'token {GHUB_TOKEN}',\n",
    "    'Accept': 'application/vnd.github.mercy-preview+json'  # Needed to access topics\n",
    "}\n",
    "\n",
    "def get_starred_repos():\n",
    "    \"\"\"Fetch all starred repositories from GitHub API\"\"\"\n",
    "    print(\"🔍 Fetching starred repositories...\")\n",
    "    starred = []\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            API_STARRED_URL, \n",
    "            headers=auth_headers, \n",
    "            params={'per_page': 100, 'page': page}\n",
    "        )\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error fetching starred repos: {response.status_code} - {response.text}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "        starred.extend(data)\n",
    "        page += 1\n",
    "        print(f\"📦 Fetched page {page-1} ({len(data)} repos)\")\n",
    "\n",
    "    print(f\"✅ Total starred repositories: {len(starred)}\")\n",
    "    return starred\n",
    "\n",
    "def get_last_release_date(owner, repo):\n",
    "    \"\"\"Get the last release date for a repository\"\"\"\n",
    "    url = API_RELEASES_URL.format(owner=owner, repo=repo)\n",
    "    response = requests.get(url, headers=auth_headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"published_at\")\n",
    "    elif response.status_code == 404:\n",
    "        return \"No releases\"\n",
    "    else:\n",
    "        return f\"Error: {response.status_code}\"\n",
    "\n",
    "def get_repo_topics(owner, repo):\n",
    "    \"\"\"Get topics for a repository\"\"\"\n",
    "    url = API_TOPICS_URL.format(owner=owner, repo=repo)\n",
    "    response = requests.get(url, headers=topics_headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get('names', [])\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def scrape_github_list(list_url, tag_name):\n",
    "    \"\"\"Scrape a single GitHub list and return repository names with tag\"\"\"\n",
    "    print(f\"🔍 Scraping list '{tag_name}': {list_url}\")\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(list_url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"❌ Failed to load page for {tag_name}: {response.status_code}\")\n",
    "            return []\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        repo_blocks = soup.select('div#user-list-repositories > div.border-bottom')\n",
    "\n",
    "        repo_names = []\n",
    "        for block in repo_blocks:\n",
    "            name_tag = block.select_one('h3 a')\n",
    "            if not name_tag:\n",
    "                continue\n",
    "                \n",
    "            full_name = name_tag['href'].strip('/')\n",
    "            repo_names.append(full_name)\n",
    "\n",
    "        print(f\"✅ Found {len(repo_names)} repositories in '{tag_name}' list\")\n",
    "        return repo_names\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error scraping {tag_name}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def get_curated_tags():\n",
    "    \"\"\"Fetch all curated lists and create a mapping of repo names to tags\"\"\"\n",
    "    print(\"🏷️ Fetching curated list tags...\")\n",
    "    repo_tags = defaultdict(set)\n",
    "    \n",
    "    for tag_name in TAG_NAMES:\n",
    "        list_config = GITHUB_LISTS[tag_name]\n",
    "        list_url = list_config['url']\n",
    "        \n",
    "        repo_names = scrape_github_list(list_url, tag_name)\n",
    "        for repo_name in repo_names:\n",
    "            repo_tags[repo_name].add(tag_name)\n",
    "        \n",
    "        # Be nice to GitHub\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(f\"✅ Collected tags for {len(repo_tags)} repositories\")\n",
    "    return repo_tags\n",
    "\n",
    "def process_repositories(repos, repo_tags):\n",
    "    \"\"\"Process repositories and gather additional data, merging with curated tags\"\"\"\n",
    "    print(\"🔄 Processing repositories and gathering additional data...\")\n",
    "    data = []\n",
    "\n",
    "    for i, repo in enumerate(repos):\n",
    "        full_name = repo['full_name']\n",
    "        owner, repo_name = full_name.split('/')\n",
    "        \n",
    "        print(f\"📊 Processing {full_name} ({i+1}/{len(repos)})\")\n",
    "        \n",
    "        # Get additional data\n",
    "        last_release = get_last_release_date(owner, repo_name)\n",
    "        topics = get_repo_topics(owner, repo_name)\n",
    "        \n",
    "        # Get curated tags for this repo\n",
    "        curated_tags = list(repo_tags.get(full_name, set()))\n",
    "        \n",
    "        # Combine GitHub topics and curated tags\n",
    "        all_tags = topics + curated_tags\n",
    "        \n",
    "        data.append({\n",
    "            'name': full_name,\n",
    "            'description': repo.get('description', ''),\n",
    "            'stars': repo['stargazers_count'],\n",
    "            'forks': repo['forks_count'],\n",
    "            'language': repo.get('language', 'Unknown'),\n",
    "            'url': repo['html_url'],\n",
    "            'last_release': last_release,\n",
    "            'topics': \", \".join(topics),\n",
    "            'curated_tags': \", \".join(sorted(curated_tags)),\n",
    "            'all_tags': \", \".join(sorted(all_tags)),\n",
    "            'tags_count': len(all_tags),\n",
    "            'is_curated': len(curated_tags) > 0,\n",
    "            'created_at': repo['created_at'],\n",
    "            'updated_at': repo['updated_at'],\n",
    "            'pushed_at': repo.get('pushed_at', ''),\n",
    "            'open_issues': repo.get('open_issues_count', 0),\n",
    "            'archived': repo.get('archived', False),\n",
    "            'fork': repo.get('fork', False),\n",
    "            'fetched_at': datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    print(f\"✅ Processed {len(data)} repositories\")\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def upload_to_google_sheet(df, sheet_name=GOOGLE_SHEET_NAME, tab_name=\"starred\"):\n",
    "    \"\"\"Upload DataFrame to Google Sheets\"\"\"\n",
    "    print(f\"📤 Uploading to Google Sheet: {sheet_name}, tab: {tab_name}\")\n",
    "    \n",
    "    # Scope for Sheets + Drive\n",
    "    scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "    \n",
    "    try:\n",
    "        # Validate credentials\n",
    "        if not GCP_CREDENTIALS:\n",
    "            raise ValueError(\"GCP_CREDENTIALS environment variable is not set\")\n",
    "        \n",
    "        # Load credentials from environment variable (JSON string)\n",
    "        if GCP_CREDENTIALS.startswith('{'):\n",
    "            # JSON string\n",
    "            creds_dict = json.loads(GCP_CREDENTIALS)\n",
    "        else:\n",
    "            # File path\n",
    "            with open(GCP_CREDENTIALS, 'r') as f:\n",
    "                creds_dict = json.load(f)\n",
    "        \n",
    "        creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)\n",
    "        client = gspread.authorize(creds)\n",
    "        \n",
    "        # Open (or create) spreadsheet\n",
    "        try:\n",
    "            sheet = client.open(sheet_name)\n",
    "        except gspread.SpreadsheetNotFound:\n",
    "            sheet = client.create(sheet_name)\n",
    "            print(f\"📝 Created new spreadsheet: {sheet_name}\")\n",
    "\n",
    "        # Try to get the worksheet, create if it doesn't exist\n",
    "        try:\n",
    "            worksheet = sheet.worksheet(tab_name)\n",
    "        except gspread.WorksheetNotFound:\n",
    "            worksheet = sheet.add_worksheet(title=tab_name, rows=\"1000\", cols=\"25\")\n",
    "            print(f\"📝 Created new worksheet: {tab_name}\")\n",
    "\n",
    "        # Clear the sheet and upload new data\n",
    "        worksheet.clear()\n",
    "        set_with_dataframe(worksheet, df)\n",
    "        \n",
    "        print(f\"✅ Uploaded {len(df)} rows to Google Sheet: {sheet_name}/{tab_name}\")\n",
    "        print(f\"🔗 Sheet URL: https://docs.google.com/spreadsheets/d/{sheet.id}\")\n",
    "        \n",
    "        return sheet.id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error uploading to Google Sheet: {type(e).__name__}: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"🚀 Starting unified starred repositories sync...\")\n",
    "    print(f\"⏰ Started at: {datetime.now().isoformat()}\")\n",
    "    \n",
    "    try:\n",
    "        # Fetch starred repositories\n",
    "        repos = get_starred_repos()\n",
    "        \n",
    "        if not repos:\n",
    "            print(\"⚠️ No starred repositories found\")\n",
    "            return\n",
    "        \n",
    "        # Get curated tags from lists\n",
    "        repo_tags = get_curated_tags()\n",
    "        \n",
    "        # Process repositories with merged data\n",
    "        df = process_repositories(repos, repo_tags)\n",
    "        \n",
    "        # Upload to Google Sheets\n",
    "        sheet_id = upload_to_google_sheet(df)\n",
    "        \n",
    "        # Print summary\n",
    "        curated_count = len(df[df['is_curated'] == True])\n",
    "        languages_count = len(df['language'].unique())\n",
    "        \n",
    "        print(f\"\\n📈 Portfolio Summary:\")\n",
    "        print(f\"   • Total starred repositories: {len(df)}\")\n",
    "        print(f\"   • Curated repositories: {curated_count}\")\n",
    "        print(f\"   • Programming languages: {languages_count}\")\n",
    "        print(f\"   • Total stars accumulated: {df['stars'].sum():,}\")\n",
    "        \n",
    "        # Show curated tag distribution\n",
    "        if curated_count > 0:\n",
    "            print(f\"   • Curated tag distribution:\")\n",
    "            for tag in TAG_NAMES:\n",
    "                tag_count = len(df[df['curated_tags'].str.contains(tag, na=False)])\n",
    "                if tag_count > 0:\n",
    "                    print(f\"     - {tag}: {tag_count} repos\")\n",
    "        \n",
    "        print(f\"\\n🎉 Successfully synced unified starred repositories!\")\n",
    "        print(f\"📊 Portfolio data available at: https://docs.google.com/spreadsheets/d/{sheet_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(f\"✅ Completed at: {datetime.now().isoformat()}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
