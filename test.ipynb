{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting unified starred repositories sync...\n",
      "‚è∞ Started at: 2025-07-03T21:49:17.027129\n",
      "üîç Fetching starred repositories...\n",
      "üì¶ Fetched page 1 (54 repos)\n",
      "‚úÖ Total starred repositories: 54\n",
      "üè∑Ô∏è Fetching curated list tags...\n",
      "üîç Scraping list 'stack': https://github.com/stars/Veatec22/lists/stack\n",
      "‚úÖ Found 14 repositories in 'stack' list\n",
      "üîç Scraping list 'nice-to-have': https://github.com/stars/Veatec22/lists/nice-to-have\n",
      "‚úÖ Found 1 repositories in 'nice-to-have' list\n",
      "üîç Scraping list 'future-ideas': https://github.com/stars/Veatec22/lists/future-ideas\n",
      "‚úÖ Found 17 repositories in 'future-ideas' list\n",
      "‚úÖ Collected tags for 32 repositories\n",
      "üîÑ Processing repositories and gathering additional data...\n",
      "üìä Processing shankarpandala/lazypredict (1/54)\n",
      "üìä Processing psf/black (2/54)\n",
      "üìä Processing pre-commit/pre-commit-hooks (3/54)\n",
      "üìä Processing pylint-dev/pylint (4/54)\n",
      "üìä Processing tadata-org/fastapi_mcp (5/54)\n",
      "üìä Processing GraphiteEditor/Graphite (6/54)\n",
      "üìä Processing dataease/dataease (7/54)\n",
      "üìä Processing getredash/redash (8/54)\n",
      "üìä Processing metabase/metabase (9/54)\n",
      "üìä Processing supabase/supabase (10/54)\n",
      "üìä Processing tensorflow/tensorflow (11/54)\n",
      "üìä Processing ml-tooling/best-of-ml-python (12/54)\n",
      "üìä Processing keras-team/keras (13/54)\n",
      "üìä Processing apache/superset (14/54)\n",
      "üìä Processing ydataai/ydata-profiling (15/54)\n",
      "üìä Processing onlook-dev/onlook (16/54)\n",
      "üìä Processing dbt-labs/dbt-core (17/54)\n",
      "üìä Processing mendableai/firecrawl (18/54)\n",
      "üìä Processing microsoft/ML-For-Beginners (19/54)\n",
      "üìä Processing sdmg15/Best-websites-a-programmer-should-visit (20/54)\n",
      "üìä Processing gitleaks/gitleaks (21/54)\n",
      "üìä Processing eyaltoledano/claude-task-master (22/54)\n",
      "üìä Processing celery/celery (23/54)\n",
      "üìä Processing python-poetry/poetry (24/54)\n",
      "üìä Processing evidence-dev/evidence (25/54)\n",
      "üìä Processing 7PH/powerglitch (26/54)\n",
      "üìä Processing lucide-icons/lucide (27/54)\n",
      "üìä Processing topoteretes/cognee (28/54)\n",
      "üìä Processing mlabonne/llm-course (29/54)\n",
      "üìä Processing optuna/optuna (30/54)\n",
      "üìä Processing coleifer/huey (31/54)\n",
      "üìä Processing unit8co/darts (32/54)\n",
      "üìä Processing modin-project/modin (33/54)\n",
      "üìä Processing dask/dask (34/54)\n",
      "üìä Processing catboost/catboost (35/54)\n",
      "üìä Processing PriorLabs/TabPFN (36/54)\n",
      "üìä Processing elephaint/pgbm (37/54)\n",
      "üìä Processing stanfordmlgroup/ngboost (38/54)\n",
      "üìä Processing LinearBoost/linearboost-classifier (39/54)\n",
      "üìä Processing ThomasMeissnerDS/BlueCast (40/54)\n",
      "üìä Processing perpetual-ml/perpetual (41/54)\n",
      "üìä Processing duckdb/duckdb (42/54)\n",
      "üìä Processing gradio-app/gradio (43/54)\n",
      "üìä Processing scikit-learn/scikit-learn (44/54)\n",
      "üìä Processing pycaret/pycaret (45/54)\n",
      "üìä Processing reflex-dev/reflex (46/54)\n",
      "üìä Processing Kanaries/pygwalker (47/54)\n",
      "üìä Processing Kanaries/graphic-walker (48/54)\n",
      "üìä Processing Avaiga/taipy (49/54)\n",
      "üìä Processing pola-rs/polars (50/54)\n",
      "üìä Processing pandas-dev/pandas (51/54)\n",
      "üìä Processing evidentlyai/evidently (52/54)\n",
      "üìä Processing streamlit/streamlit (53/54)\n",
      "üìä Processing autogluon/autogluon (54/54)\n",
      "‚úÖ Processed 54 repositories\n",
      "üì§ Uploading to Google Sheet: github_data, tab: starred\n",
      "‚úÖ Uploaded 54 rows to Google Sheet: github_data/starred\n",
      "üîó Sheet URL: https://docs.google.com/spreadsheets/d/1sC--EoeGVjOfcKjeI5U9jL55nN0LhZXD4sepUTqCE20\n",
      "\n",
      "üìà Portfolio Summary:\n",
      "   ‚Ä¢ Total starred repositories: 54\n",
      "   ‚Ä¢ Curated repositories: 32\n",
      "   ‚Ä¢ Programming languages: 11\n",
      "   ‚Ä¢ Total stars accumulated: 1,416,245\n",
      "   ‚Ä¢ Curated tag distribution:\n",
      "     - stack: 14 repos\n",
      "     - nice-to-have: 1 repos\n",
      "     - future-ideas: 17 repos\n",
      "\n",
      "üéâ Successfully synced unified starred repositories!\n",
      "üìä Portfolio data available at: https://docs.google.com/spreadsheets/d/1sC--EoeGVjOfcKjeI5U9jL55nN0LhZXD4sepUTqCE20\n",
      "‚úÖ Completed at: 2025-07-03T21:50:08.877720\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Unified Starred GitHub Repositories Fetcher\n",
    "Fetches starred repositories from GitHub API and merges with curated list tags\n",
    "Creates a comprehensive portfolio view combining detailed repo data with tag organization\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import our lists configuration  \n",
    "# GitHub Lists Configuration\n",
    "# This file contains the dictionary of GitHub starred lists to be used as tags\n",
    "\n",
    "GITHUB_LISTS = {\n",
    "    \"stack\": {\n",
    "        \"url\": \"https://github.com/stars/Veatec22/lists/stack\",\n",
    "        \"description\": \"Core development stack and essential tools\"\n",
    "    },\n",
    "    \"nice-to-have\": {\n",
    "        \"url\": \"https://github.com/stars/Veatec22/lists/nice-to-have\", \n",
    "        \"description\": \"Useful tools and libraries for future consideration\"\n",
    "    },\n",
    "    \"future-ideas\": {\n",
    "        \"url\": \"https://github.com/stars/Veatec22/lists/future-ideas\",\n",
    "        \"description\": \"Innovative projects and experimental technologies\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# List of tag names for easy iteration\n",
    "TAG_NAMES = list(GITHUB_LISTS.keys())\n",
    "\n",
    "# Default sheet tab name for the unified starred data\n",
    "STARRED_SHEET_TAB = \"starred\"\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "load_dotenv()\n",
    "GHUB_TOKEN = os.getenv('GHUB_TOKEN')\n",
    "GCP_CREDENTIALS = os.getenv('GCP_CREDENTIALS')\n",
    "GOOGLE_SHEET_NAME = os.getenv('GOOGLE_SHEET_NAME')\n",
    "GOOGLE_SHEET_ID = os.getenv('GOOGLE_SHEET_ID')\n",
    "\n",
    "# GitHub API endpoints\n",
    "API_STARRED_URL = 'https://api.github.com/user/starred'\n",
    "API_RELEASES_URL = 'https://api.github.com/repos/{owner}/{repo}/releases/latest'\n",
    "API_TOPICS_URL = 'https://api.github.com/repos/{owner}/{repo}/topics'\n",
    "\n",
    "auth_headers = {\n",
    "    'Authorization': f'token {GHUB_TOKEN}',\n",
    "    'Accept': 'application/vnd.github.v3+json'\n",
    "}\n",
    "\n",
    "topics_headers = {\n",
    "    'Authorization': f'token {GHUB_TOKEN}',\n",
    "    'Accept': 'application/vnd.github.mercy-preview+json'  # Needed to access topics\n",
    "}\n",
    "\n",
    "def get_starred_repos():\n",
    "    \"\"\"Fetch all starred repositories from GitHub API\"\"\"\n",
    "    print(\"üîç Fetching starred repositories...\")\n",
    "    starred = []\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            API_STARRED_URL, \n",
    "            headers=auth_headers, \n",
    "            params={'per_page': 100, 'page': page}\n",
    "        )\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error fetching starred repos: {response.status_code} - {response.text}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "        starred.extend(data)\n",
    "        page += 1\n",
    "        print(f\"üì¶ Fetched page {page-1} ({len(data)} repos)\")\n",
    "\n",
    "    print(f\"‚úÖ Total starred repositories: {len(starred)}\")\n",
    "    return starred\n",
    "\n",
    "def get_last_release_date(owner, repo):\n",
    "    \"\"\"Get the last release date for a repository\"\"\"\n",
    "    url = API_RELEASES_URL.format(owner=owner, repo=repo)\n",
    "    response = requests.get(url, headers=auth_headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"published_at\")\n",
    "    elif response.status_code == 404:\n",
    "        return \"No releases\"\n",
    "    else:\n",
    "        return f\"Error: {response.status_code}\"\n",
    "\n",
    "def get_repo_topics(owner, repo):\n",
    "    \"\"\"Get topics for a repository\"\"\"\n",
    "    url = API_TOPICS_URL.format(owner=owner, repo=repo)\n",
    "    response = requests.get(url, headers=topics_headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get('names', [])\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def scrape_github_list(list_url, tag_name):\n",
    "    \"\"\"Scrape a single GitHub list and return repository names with tag\"\"\"\n",
    "    print(f\"üîç Scraping list '{tag_name}': {list_url}\")\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(list_url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Failed to load page for {tag_name}: {response.status_code}\")\n",
    "            return []\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        repo_blocks = soup.select('div#user-list-repositories > div.border-bottom')\n",
    "\n",
    "        repo_names = []\n",
    "        for block in repo_blocks:\n",
    "            name_tag = block.select_one('h3 a')\n",
    "            if not name_tag:\n",
    "                continue\n",
    "                \n",
    "            full_name = name_tag['href'].strip('/')\n",
    "            repo_names.append(full_name)\n",
    "\n",
    "        print(f\"‚úÖ Found {len(repo_names)} repositories in '{tag_name}' list\")\n",
    "        return repo_names\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error scraping {tag_name}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def get_curated_tags():\n",
    "    \"\"\"Fetch all curated lists and create a mapping of repo names to tags\"\"\"\n",
    "    print(\"üè∑Ô∏è Fetching curated list tags...\")\n",
    "    repo_tags = defaultdict(set)\n",
    "    \n",
    "    for tag_name in TAG_NAMES:\n",
    "        list_config = GITHUB_LISTS[tag_name]\n",
    "        list_url = list_config['url']\n",
    "        \n",
    "        repo_names = scrape_github_list(list_url, tag_name)\n",
    "        for repo_name in repo_names:\n",
    "            repo_tags[repo_name].add(tag_name)\n",
    "        \n",
    "        # Be nice to GitHub\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(f\"‚úÖ Collected tags for {len(repo_tags)} repositories\")\n",
    "    return repo_tags\n",
    "\n",
    "def process_repositories(repos, repo_tags):\n",
    "    \"\"\"Process repositories and gather additional data, merging with curated tags\"\"\"\n",
    "    print(\"üîÑ Processing repositories and gathering additional data...\")\n",
    "    data = []\n",
    "\n",
    "    for i, repo in enumerate(repos):\n",
    "        full_name = repo['full_name']\n",
    "        owner, repo_name = full_name.split('/')\n",
    "        \n",
    "        print(f\"üìä Processing {full_name} ({i+1}/{len(repos)})\")\n",
    "        \n",
    "        # Get additional data\n",
    "        last_release = get_last_release_date(owner, repo_name)\n",
    "        topics = get_repo_topics(owner, repo_name)\n",
    "        \n",
    "        # Get curated tags for this repo\n",
    "        curated_tags = list(repo_tags.get(full_name, set()))\n",
    "        \n",
    "        # Combine GitHub topics and curated tags\n",
    "        all_tags = topics + curated_tags\n",
    "        \n",
    "        data.append({\n",
    "            'name': full_name,\n",
    "            'description': repo.get('description', ''),\n",
    "            'stars': repo['stargazers_count'],\n",
    "            'forks': repo['forks_count'],\n",
    "            'language': repo.get('language', 'Unknown'),\n",
    "            'url': repo['html_url'],\n",
    "            'last_release': last_release,\n",
    "            'topics': \", \".join(topics),\n",
    "            'curated_tags': \", \".join(sorted(curated_tags)),\n",
    "            'all_tags': \", \".join(sorted(all_tags)),\n",
    "            'tags_count': len(all_tags),\n",
    "            'is_curated': len(curated_tags) > 0,\n",
    "            'created_at': repo['created_at'],\n",
    "            'updated_at': repo['updated_at'],\n",
    "            'pushed_at': repo.get('pushed_at', ''),\n",
    "            'open_issues': repo.get('open_issues_count', 0),\n",
    "            'archived': repo.get('archived', False),\n",
    "            'fork': repo.get('fork', False),\n",
    "            'fetched_at': datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    print(f\"‚úÖ Processed {len(data)} repositories\")\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def upload_to_google_sheet(df, sheet_name=GOOGLE_SHEET_NAME, tab_name=\"starred\"):\n",
    "    \"\"\"Upload DataFrame to Google Sheets\"\"\"\n",
    "    print(f\"üì§ Uploading to Google Sheet: {sheet_name}, tab: {tab_name}\")\n",
    "    \n",
    "    # Scope for Sheets + Drive\n",
    "    scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "    \n",
    "    try:\n",
    "        # Validate credentials\n",
    "        if not GCP_CREDENTIALS:\n",
    "            raise ValueError(\"GCP_CREDENTIALS environment variable is not set\")\n",
    "        \n",
    "        # Load credentials from environment variable (JSON string)\n",
    "        if GCP_CREDENTIALS.startswith('{'):\n",
    "            # JSON string\n",
    "            creds_dict = json.loads(GCP_CREDENTIALS)\n",
    "        else:\n",
    "            # File path\n",
    "            with open(GCP_CREDENTIALS, 'r') as f:\n",
    "                creds_dict = json.load(f)\n",
    "        \n",
    "        creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)\n",
    "        client = gspread.authorize(creds)\n",
    "        \n",
    "        # Open (or create) spreadsheet\n",
    "        try:\n",
    "            sheet = client.open(sheet_name)\n",
    "        except gspread.SpreadsheetNotFound:\n",
    "            sheet = client.create(sheet_name)\n",
    "            print(f\"üìù Created new spreadsheet: {sheet_name}\")\n",
    "\n",
    "        # Try to get the worksheet, create if it doesn't exist\n",
    "        try:\n",
    "            worksheet = sheet.worksheet(tab_name)\n",
    "        except gspread.WorksheetNotFound:\n",
    "            worksheet = sheet.add_worksheet(title=tab_name, rows=\"1000\", cols=\"25\")\n",
    "            print(f\"üìù Created new worksheet: {tab_name}\")\n",
    "\n",
    "        # Clear the sheet and upload new data\n",
    "        worksheet.clear()\n",
    "        set_with_dataframe(worksheet, df)\n",
    "        \n",
    "        print(f\"‚úÖ Uploaded {len(df)} rows to Google Sheet: {sheet_name}/{tab_name}\")\n",
    "        print(f\"üîó Sheet URL: https://docs.google.com/spreadsheets/d/{sheet.id}\")\n",
    "        \n",
    "        return sheet.id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error uploading to Google Sheet: {type(e).__name__}: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"üöÄ Starting unified starred repositories sync...\")\n",
    "    print(f\"‚è∞ Started at: {datetime.now().isoformat()}\")\n",
    "    \n",
    "    try:\n",
    "        # Fetch starred repositories\n",
    "        repos = get_starred_repos()\n",
    "        \n",
    "        if not repos:\n",
    "            print(\"‚ö†Ô∏è No starred repositories found\")\n",
    "            return\n",
    "        \n",
    "        # Get curated tags from lists\n",
    "        repo_tags = get_curated_tags()\n",
    "        \n",
    "        # Process repositories with merged data\n",
    "        df = process_repositories(repos, repo_tags)\n",
    "        \n",
    "        # Upload to Google Sheets\n",
    "        sheet_id = upload_to_google_sheet(df)\n",
    "        \n",
    "        # Print summary\n",
    "        curated_count = len(df[df['is_curated'] == True])\n",
    "        languages_count = len(df['language'].unique())\n",
    "        \n",
    "        print(f\"\\nüìà Portfolio Summary:\")\n",
    "        print(f\"   ‚Ä¢ Total starred repositories: {len(df)}\")\n",
    "        print(f\"   ‚Ä¢ Curated repositories: {curated_count}\")\n",
    "        print(f\"   ‚Ä¢ Programming languages: {languages_count}\")\n",
    "        print(f\"   ‚Ä¢ Total stars accumulated: {df['stars'].sum():,}\")\n",
    "        \n",
    "        # Show curated tag distribution\n",
    "        if curated_count > 0:\n",
    "            print(f\"   ‚Ä¢ Curated tag distribution:\")\n",
    "            for tag in TAG_NAMES:\n",
    "                tag_count = len(df[df['curated_tags'].str.contains(tag, na=False)])\n",
    "                if tag_count > 0:\n",
    "                    print(f\"     - {tag}: {tag_count} repos\")\n",
    "        \n",
    "        print(f\"\\nüéâ Successfully synced unified starred repositories!\")\n",
    "        print(f\"üìä Portfolio data available at: https://docs.google.com/spreadsheets/d/{sheet_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(f\"‚úÖ Completed at: {datetime.now().isoformat()}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
